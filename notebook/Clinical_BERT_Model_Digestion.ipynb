{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import trange, tqdm\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/share/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import interp\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.utils.fixes import signature\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/anaconda3/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the modeling class\n",
      "in the modeling class\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "#important\n",
    "from modeling_readmission import BertForSequenceClassification\n",
    "print('in the modeling class')\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError() #this is a runtime error\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    # classmethod decoration, doesn't need to sepecify self in the defination\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "        \n",
    "    @classmethod\n",
    "    def _read_csv(cls, input_file):\n",
    "        \"\"\"Reads a comma separated value file.\"\"\"\n",
    "        file=pd.read_csv(input_file)\n",
    "        lines=zip(file.ID,file.TEXT,file.Label)\n",
    "        return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "#help(builtins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class readmissionProcessor(DataProcessor):\n",
    "    def get_train_examples(self, data_dir):\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.csv\")))\n",
    "        return self._create_examples(\n",
    "            self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\")\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_csv(os.path.join(data_dir, \"val.csv\")), \"val\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_csv(os.path.join(data_dir, \"test.csv\")), \"test\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\"]\n",
    "        \n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i) \n",
    "            text_a = line[1]\n",
    "            label = str(int(line[2])) \n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -1 /data/users/haotiang/20211217_num2word/3days/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_path = '/data/users/haotiang/20211217_num2word/3days/train.csv'\n",
    "for i, line in enumerate(DataProcessor._read_csv(test_train_path)):\n",
    "    if i < 2:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_path = '/data/users/haotiang/20211217_num2word/3days/train.csv'\n",
    "examples = []\n",
    "for i, line in enumerate(DataProcessor._read_csv(test_train_path)):\n",
    "    if i < 2:\n",
    "        guid = \"%s-%s\" % (\"test\", i) \n",
    "        text_a = line[1] # get text\n",
    "        label = str(int(line[2])) # get label\n",
    "        examples.append((guid, text_a, label))\n",
    "print(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_path = '/data/users/haotiang/20211217_num2word/3days/train.csv'\n",
    "examples = []\n",
    "for i, line in enumerate(DataProcessor._read_csv(test_train_path)):\n",
    "    guid = \"%s-%s\" % (\"test\", i) \n",
    "    text_a = line[1] # get text\n",
    "    label = str(int(line[2])) # get label\n",
    "    examples.append(\n",
    "        InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:14:18 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/haotiang/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b) # text_b is not useful in our task\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(max_seq_length - 2)] # only trained with 0: 512-2\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens) \n",
    "        # BertTokenizer.from_pretrained('bert-base-uncased').convert_tokens_to_ids\n",
    "        # convert tokens to number (max 512)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        #print (example.label)\n",
    "        label_id = label_map[example.label]\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "    label_map[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -1 /data/users/haotiang/20211217_num2word/3days/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_file = '/data/users/haotiang/20211217_num2word/3days/train.csv'\n",
    "pd_train = pd.read_csv(train_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train.sort_values(by=['ID']).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples[0].text_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples[0].guid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the first line in train.csv under 3days folder\n",
    "features = []\n",
    "trial_example = train_examples[0]\n",
    "tokens_a = tokenizer.tokenize(trial_example.text_a)\n",
    "tokens_b = None\n",
    "if trial_example.text_b:\n",
    "    tokens_b = tokenizer.tokenize(trial_example.text_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_example.text_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "test_train = []\n",
    "for (ex_index, example) in enumerate(train_examples):\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    test_train.append((example.guid, tokens_a, tokens_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_1 = pd.DataFrame(test_train)\n",
    "pd_1['len'] = pd_1[1].map(lambda x: len(x))\n",
    "pd_1.sort_values(by=['len'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_1[pd_1['len'] < 512].sort_values(by=['len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_1[pd_1['len'] < 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "test_train = []\n",
    "for (ex_index, example) in enumerate(train_examples):\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    test_train.append((example.guid, tokens_a, tokens_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(test_train[0][1][: 512-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:15:33 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   guid: train-0\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   tokens: [CLS] the degree of hem ##or ##rh ##age appears relatively stable since the recent prior examination . areas of basil ##ar consolidation identified as on prior study . ( over ) four : forty - two pm ct ##a abd w & w / o c & rec ##ons ; ct pe ##lvis w / contrast clip # reason : please eva ##l hem ##ang ##iom ##a , liver bleeding , other abd pathology . admitting diagnosis : gi bleed ; tel ##eme ##try field of view : forty contrast : visa ##pa ##que am ##t : one hundred and fifty final report ( con ##t ) swan ##a status d : admitted from ed oriented x ##3 . . c / o abd pain with breathing or moving abd di ##sten ##ded firm . . o ##2 dc ' d by ho . . iv d ##5 ##w w / bi ##car ##b @ 100 ##cc foley with ad ##qua ##te hu ##o r : vs ##s p : will continue to monitor closely for further bleeding . . pc ' s per ho update o : see care ##vu ##e for specific ##s . ne ##uro : awake alert oriented , c / o of mod severe abd pain w rep ##osition & turning . med w ms ##o ##4 2 ##mg - 4 ##mg iv and rep ##osition ##ed w relief of pain . cv : skin - , and dry . sr rare pv ##c sb ##p stable to ##l dil ##tia ##ze ##m as ordered . res ##p : sat ##s ninety - two - ninety - three % on rm air - > np @ three lp ##m w sat ##s improved to ninety - seven % . bb ##s clear dim ##ins ##h bi ##bas ( most likely d / t limited chest excursion 2nd ##ary abd firm di ##sten ##ted ) . gi : abd tender firm di ##sten ##ded . no n / v . bow ##el s ##nds + . gi prop ##hyl ##ax w pp ##i . to ##l po cl li ##q ##s well . glucose > one hundred and fifty r ##x w ri ##ss . gu : q ##s to b ##dl ##ine amber urine via foley cat ##h . iv ##f ' s at 100 ##cc / hr w nah ##co ##3 one hundred and fifty me ##q per liter . dr updated w low u ##op ' s , watching for now . hem ##e / id : rec ' d 2 ##up ##rb ##c hc ##t twenty - four - twenty - six after trans ##fusion . additional repeat hc ##t w am labs pending . t ##max one hundred . seven ps ##ych / soc / family : pt has one da ##ught one son - > in to visit w pt on ad ##m . daughter is spokesperson , phone update given to same . a / p : an ##emia , cycle hc ##t ' s to r / o intra [SEP]\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   input_ids: 101 1996 3014 1997 19610 2953 25032 4270 3544 4659 6540 2144 1996 3522 3188 7749 1012 2752 1997 14732 2906 17439 4453 2004 2006 3188 2817 1012 1006 2058 1007 2176 1024 5659 1011 2048 7610 14931 2050 19935 1059 1004 1059 1013 1051 1039 1004 28667 5644 1025 14931 21877 28530 1059 1013 5688 12528 1001 3114 1024 3531 9345 2140 19610 5654 18994 2050 1010 11290 9524 1010 2060 19935 19314 1012 17927 11616 1024 21025 19501 1025 10093 21382 11129 2492 1997 3193 1024 5659 5688 1024 9425 4502 4226 2572 2102 1024 2028 3634 1998 5595 2345 3189 1006 9530 2102 1007 10677 2050 3570 1040 1024 4914 2013 3968 8048 1060 2509 1012 1012 1039 1013 1051 19935 3255 2007 5505 2030 3048 19935 4487 16173 5732 3813 1012 1012 1051 2475 5887 1005 1040 2011 7570 1012 1012 4921 1040 2629 2860 1059 1013 12170 10010 2497 1030 2531 9468 17106 2007 4748 16211 2618 15876 2080 1054 1024 5443 2015 1052 1024 2097 3613 2000 8080 4876 2005 2582 9524 1012 1012 7473 1005 1055 2566 7570 10651 1051 1024 2156 2729 19722 2063 2005 3563 2015 1012 11265 10976 1024 8300 9499 8048 1010 1039 1013 1051 1997 16913 5729 19935 3255 1059 16360 19234 1004 3810 1012 19960 1059 5796 2080 2549 1016 24798 1011 1018 24798 4921 1998 16360 19234 2098 1059 4335 1997 3255 1012 26226 1024 3096 1011 1010 1998 4318 1012 5034 4678 26189 2278 24829 2361 6540 2000 2140 29454 10711 4371 2213 2004 3641 1012 24501 2361 1024 2938 2015 13568 1011 2048 1011 13568 1011 2093 1003 2006 28549 2250 1011 1028 27937 1030 2093 6948 2213 1059 2938 2015 5301 2000 13568 1011 2698 1003 1012 22861 2015 3154 11737 7076 2232 12170 22083 1006 2087 3497 1040 1013 1056 3132 3108 26144 3416 5649 19935 3813 4487 16173 3064 1007 1012 21025 1024 19935 8616 3813 4487 16173 5732 1012 2053 1050 1013 1058 1012 6812 2884 1055 18376 1009 1012 21025 17678 29598 8528 1059 4903 2072 1012 2000 2140 13433 18856 5622 4160 2015 2092 1012 18423 1028 2028 3634 1998 5595 1054 2595 1059 15544 4757 1012 19739 1024 1053 2015 2000 1038 19422 3170 8994 17996 3081 17106 4937 2232 1012 4921 2546 1005 1055 2012 2531 9468 1013 17850 1059 20976 3597 2509 2028 3634 1998 5595 2033 4160 2566 23675 1012 2852 7172 1059 2659 1057 7361 1005 1055 1010 3666 2005 2085 1012 19610 2063 1013 8909 1024 28667 1005 1040 1016 6279 15185 2278 16731 2102 3174 1011 2176 1011 3174 1011 2416 2044 9099 20523 1012 3176 9377 16731 2102 1059 2572 13625 14223 1012 1056 17848 2028 3634 1012 2698 8827 17994 1013 27084 1013 2155 1024 13866 2038 2028 4830 18533 2028 2365 1011 1028 1999 2000 3942 1059 13866 2006 4748 2213 1012 2684 2003 15974 1010 3042 10651 2445 2000 2168 1012 1037 1013 1052 1024 2019 17577 1010 5402 16731 2102 1005 1055 2000 1054 1013 1051 26721 102\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   label: 0 (id = 0)\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   guid: train-1\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   tokens: [CLS] pt request in ##hale ##rs when c / o breathing difficulties , assisted w / al ##bu ##terol and fl ##oven ##t , using space ##r . this rn inquired as to pt frequency of overnight in ##hale ##r needs , response : breathing he ##avi ##ness relieved after in ##hale ##r use . pt states that he awake ##ns every night between one and 5 ##am and takes two puff ##s of al ##bu ##terol . plan : con ##t to assess for cp , sob , continuing to cycle cardiac enzymes ; ec ##g to be repeated at ##rial fi ##bri ##llation ( afi ##b ) assessment : has remained in sr , no evidence of afi ##b action : cardiac monitoring response : remained in sr , plan : con ##t to monitor for afi ##b . [SEP]\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   input_ids: 101 13866 5227 1999 15238 2869 2043 1039 1013 1051 5505 8190 1010 7197 1059 1013 2632 8569 27833 1998 13109 25592 2102 1010 2478 2686 2099 1012 2023 29300 24849 2004 2000 13866 6075 1997 11585 1999 15238 2099 3791 1010 3433 1024 5505 2002 18891 2791 7653 2044 1999 15238 2099 2224 1012 13866 2163 2008 2002 8300 3619 2296 2305 2090 2028 1998 1019 3286 1998 3138 2048 23893 2015 1997 2632 8569 27833 1012 2933 1024 9530 2102 2000 14358 2005 18133 1010 17540 1010 5719 2000 5402 15050 16285 1025 14925 2290 2000 2022 5567 2012 14482 10882 23736 20382 1006 28697 2497 1007 7667 1024 2038 2815 1999 5034 1010 2053 3350 1997 28697 2497 2895 1024 15050 8822 3433 1024 2815 1999 5034 1010 2933 1024 9530 2102 2000 8080 2005 28697 2497 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:15:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   label: 0 (id = 0)\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   guid: train-2\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   tokens: [CLS] thick ##ened . there is no mit ##ral valve pro ##la ##pse . mild ( one + ) mit ##ral reg ##urg ##itation is seen . there is mild pulmonary artery sy ##sto ##lic hyper ##tension . there is no per ##ica ##rdial e ##ff ##usion . impression : sub ##op ##ti ##mal image quality . mild symmetric left vent ##ric ##ular hyper ##tro ##phy with preserved global and regional bi ##vent ##ric ##ular sy ##sto ##lic function . mild pulmonary artery sy ##sto ##lic hyper ##tension . mild mit ##ral reg ##urg ##itation . compared with the prior study ( images reviewed ) of , the estimated pulmonary artery sy ##sto ##lic pressure is now lower . the right vent ##ric ##ular cavity is also smaller / now normal . clinical implications : based on ah ##a end ##oca ##rdi ##tis prop ##hyl ##ax ##is recommendations , the echo findings indicate prop ##hyl ##ax ##is is not recommended . clinical decisions regarding the need for prop ##hyl ##ax ##is should be based on clinical and echo ##card ##io ##graphic data . possible wandering at ##rial pace ##maker versus sin ##us rhythm with premature at ##rial contraction ##s and first degree a - v block . right bundle - branch block . non - specific st - t wave changes . compared to tracing # three no significant change . tracing # four possible wandering at ##rial pace ##maker with first degree a - v block versus sin ##us rhythm with premature at ##rial contraction ##s . right bundle - branch block . non - specific st - t wave changes . compared to tracing # two no significant change . tracing # three respiratory failure , acute ( not ar ##ds / ) assessment : patient received on bi ##pa ##p this am , sat ##s ninety - five % , ta ##chy ##p ##ne ##ic with rr > twenty - five , ab ##g s with respiratory acid ##osis action : patient very agitated on bi ##pa ##p , with slight improvement on ab ##g , bi ##pa ##p removed and placed on four liter ##s nc response : to ##ler ##ating well , with only slight dip in ab ##g ( see meta ##vision for details ) patient remains on nc , calm and cooperative , serial ab ##g throughout afternoon to monitor status plan : placed back on bi ##pa ##p due to worse ##ning ab ##g , monitor throughout the evening and overnight . deep ve ##nous th ##rom ##bos ##is ( d ##v ##t ) , upper ex ##tre ##mity assessment : patient known to have history of d ##v ##t in lu ##e action : us obtained at bedside of lu ##e , tt ##e obtained to monitor for rv strain response : md tt ##e showed e ##f fifty - five % with mild l ##v ##h , awaiting us results plan : no blood draws or ni ##b ##p in le hyper ##kal ##emia ( high potassium , hyper ##pot ##asse [SEP]\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   input_ids: 101 4317 6675 1012 2045 2003 2053 10210 7941 10764 4013 2721 29251 1012 10256 1006 2028 1009 1007 10210 7941 19723 12514 18557 2003 2464 1012 2045 2003 10256 21908 16749 25353 16033 10415 23760 29048 1012 2045 2003 2053 2566 5555 25070 1041 4246 14499 1012 8605 1024 4942 7361 3775 9067 3746 3737 1012 10256 19490 2187 18834 7277 7934 23760 13181 21281 2007 6560 3795 1998 3164 12170 15338 7277 7934 25353 16033 10415 3853 1012 10256 21908 16749 25353 16033 10415 23760 29048 1012 10256 10210 7941 19723 12514 18557 1012 4102 2007 1996 3188 2817 1006 4871 8182 1007 1997 1010 1996 4358 21908 16749 25353 16033 10415 3778 2003 2085 2896 1012 1996 2157 18834 7277 7934 17790 2003 2036 3760 1013 2085 3671 1012 6612 13494 1024 2241 2006 6289 2050 2203 24755 17080 7315 17678 29598 8528 2483 11433 1010 1996 9052 9556 5769 17678 29598 8528 2483 2003 2025 6749 1012 6612 6567 4953 1996 2342 2005 17678 29598 8528 2483 2323 2022 2241 2006 6612 1998 9052 11522 3695 14773 2951 1012 2825 13071 2012 14482 6393 8571 6431 8254 2271 6348 2007 21371 2012 14482 21963 2015 1998 2034 3014 1037 1011 1058 3796 1012 2157 14012 1011 3589 3796 1012 2512 1011 3563 2358 1011 1056 4400 3431 1012 4102 2000 16907 1001 2093 2053 3278 2689 1012 16907 1001 2176 2825 13071 2012 14482 6393 8571 2007 2034 3014 1037 1011 1058 3796 6431 8254 2271 6348 2007 21371 2012 14482 21963 2015 1012 2157 14012 1011 3589 3796 1012 2512 1011 3563 2358 1011 1056 4400 3431 1012 4102 2000 16907 1001 2048 2053 3278 2689 1012 16907 1001 2093 16464 4945 1010 11325 1006 2025 12098 5104 1013 1007 7667 1024 5776 2363 2006 12170 4502 2361 2023 2572 1010 2938 2015 13568 1011 2274 1003 1010 11937 11714 2361 2638 2594 2007 25269 1028 3174 1011 2274 1010 11113 2290 1055 2007 16464 5648 12650 2895 1024 5776 2200 21568 2006 12170 4502 2361 1010 2007 7263 7620 2006 11113 2290 1010 12170 4502 2361 3718 1998 2872 2006 2176 23675 2015 13316 3433 1024 2000 3917 5844 2092 1010 2007 2069 7263 16510 1999 11113 2290 1006 2156 18804 17084 2005 4751 1007 5776 3464 2006 13316 1010 5475 1998 10791 1010 7642 11113 2290 2802 5027 2000 8080 3570 2933 1024 2872 2067 2006 12170 4502 2361 2349 2000 4788 5582 11113 2290 1010 8080 2802 1996 3944 1998 11585 1012 2784 2310 18674 16215 21716 15853 2483 1006 1040 2615 2102 1007 1010 3356 4654 7913 16383 7667 1024 5776 2124 2000 2031 2381 1997 1040 2615 2102 1999 11320 2063 2895 1024 2149 4663 2012 19475 1997 11320 2063 1010 23746 2063 4663 2000 8080 2005 27634 10178 3433 1024 9108 23746 2063 3662 1041 2546 5595 1011 2274 1003 2007 10256 1048 2615 2232 1010 15497 2149 3463 2933 1024 2053 2668 9891 2030 9152 2497 2361 1999 3393 23760 12902 17577 1006 2152 18044 1010 23760 11008 27241 102\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:15:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   label: 1 (id = 1)\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   guid: train-3\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   tokens: [CLS] good air movement bi ##ater ##ally . abd : + bs , soft tt ##p throughout but mostly ep ##iga ##st ##ric . no rebound or guarding . ex ##t : no ed ##ema . right fe ##moral hd line non ##ten ##der , none ##ry ##the ##mat ##ous . ne ##uro : a & ox ##3 . appropriate . cn two - twelve gross ##ly intact . preserved sensation to light touch throughout . five / five strength in her upper and lower ex ##tre ##mit ##ies labs / radio ##logy : head ct read as negative thirteen . one g / dl one hundred and seventy - five k / ul one hundred and twenty - two mg / dl seven . nine mg / dl twenty - three me ##q / l six . four me ##q / l fifty - two mg / dl one hundred and one me ##q / l one hundred and thirty - eight me ##q / l forty . five % eight . seven k / ul eight : fifty - five pm nine : thirteen pm five : twenty - nine am wb ##c seven . zero six . eight eight . seven hc ##t thirty - four . two thirty - three . seven forty . five pl ##t one hundred and twenty - six one hundred and forty - five one hundred and seventy - five cr seven . seven seven . nine glucose ninety - eight one hundred and twenty - two other labs : pt / pt ##t / in ##r : fourteen . five / thirty . one / one . three , lac ##tic acid : zero . nine mm ##ol / l , ca + + : nine . six mg / dl , mg + + : two . two mg / dl , po ##4 : seven . one mg / dl assessment and plan twenty - four yo female with es ##rd on hd , mali ##gnant hyper ##tension with h ##x of intra ##cer ##eb ##ral hem ##or ##rh ##age , sl ##e , chronic abdominal pain , and sv ##c syndrome admitted due to hyper ##tens ##ive urgency after developing n / v and being unable to take her po medications . # hyper ##tens ##ive urgency : - continue out ##patient medications - ti ##tra ##te off nic ##ard ##ap ##ine drip - hydra ##la ##zine pr ##n # abdominal pain / dia ##rr ##hea : - chronic issue did report pain improved this morning . - will clarify how much pain med ##s she is taking as na ##rco ##tic withdraw ##l could explain these sy ##mt ##oms - will ask gi to weigh to see if she would benefit from si ##gm ##oid ##os ##co ##py # nausea / vomiting : - continue pr ##n med ##s - no iv dil ##aud ##ed per pc ##p hc ##g serum # es ##rd on hd : - will dial ##ize today - continue renal diet and [SEP]\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   input_ids: 101 2204 2250 2929 12170 24932 3973 1012 19935 1024 1009 18667 1010 3730 23746 2361 2802 2021 3262 4958 13340 3367 7277 1012 2053 27755 2030 17019 1012 4654 2102 1024 2053 3968 14545 1012 2157 10768 22049 10751 2240 2512 6528 4063 1010 3904 2854 10760 18900 3560 1012 11265 10976 1024 1037 1004 23060 2509 1012 6413 1012 27166 2048 1011 4376 7977 2135 10109 1012 6560 8742 2000 2422 3543 2802 1012 2274 1013 2274 3997 1999 2014 3356 1998 2896 4654 7913 22930 3111 13625 1013 2557 6483 1024 2132 14931 3191 2004 4997 7093 1012 2028 1043 1013 21469 2028 3634 1998 10920 1011 2274 1047 1013 17359 2028 3634 1998 3174 1011 2048 11460 1013 21469 2698 1012 3157 11460 1013 21469 3174 1011 2093 2033 4160 1013 1048 2416 1012 2176 2033 4160 1013 1048 5595 1011 2048 11460 1013 21469 2028 3634 1998 2028 2033 4160 1013 1048 2028 3634 1998 4228 1011 2809 2033 4160 1013 1048 5659 1012 2274 1003 2809 1012 2698 1047 1013 17359 2809 1024 5595 1011 2274 7610 3157 1024 7093 7610 2274 1024 3174 1011 3157 2572 25610 2278 2698 1012 5717 2416 1012 2809 2809 1012 2698 16731 2102 4228 1011 2176 1012 2048 4228 1011 2093 1012 2698 5659 1012 2274 20228 2102 2028 3634 1998 3174 1011 2416 2028 3634 1998 5659 1011 2274 2028 3634 1998 10920 1011 2274 13675 2698 1012 2698 2698 1012 3157 18423 13568 1011 2809 2028 3634 1998 3174 1011 2048 2060 13625 1024 13866 1013 13866 2102 1013 1999 2099 1024 7426 1012 2274 1013 4228 1012 2028 1013 2028 1012 2093 1010 18749 4588 5648 1024 5717 1012 3157 3461 4747 1013 1048 1010 6187 1009 1009 1024 3157 1012 2416 11460 1013 21469 1010 11460 1009 1009 1024 2048 1012 2048 11460 1013 21469 1010 13433 2549 1024 2698 1012 2028 11460 1013 21469 7667 1998 2933 3174 1011 2176 10930 2931 2007 9686 4103 2006 10751 1010 16007 27881 23760 29048 2007 1044 2595 1997 26721 17119 15878 7941 19610 2953 25032 4270 1010 22889 2063 1010 11888 21419 3255 1010 1998 17917 2278 8715 4914 2349 2000 23760 25808 3512 19353 2044 4975 1050 1013 1058 1998 2108 4039 2000 2202 2014 13433 20992 1012 1001 23760 25808 3512 19353 1024 1011 3613 2041 24343 20992 1011 14841 6494 2618 2125 27969 4232 9331 3170 27304 1011 26018 2721 21254 10975 2078 1001 21419 3255 1013 22939 12171 20192 1024 1011 11888 3277 2106 3189 3255 5301 2023 2851 1012 1011 2097 25037 2129 2172 3255 19960 2015 2016 2003 2635 2004 6583 29566 4588 10632 2140 2071 4863 2122 25353 20492 22225 1011 2097 3198 21025 2000 17042 2000 2156 2065 2016 2052 5770 2013 9033 21693 9314 2891 3597 7685 1001 19029 1013 24780 1024 1011 3613 10975 2078 19960 2015 1011 2053 4921 29454 19513 2098 2566 7473 2361 16731 2290 20194 1001 9686 4103 2006 10751 1024 1011 2097 13764 4697 2651 1011 3613 25125 8738 1998 102\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:15:33 - INFO - __main__ -   label: 1 (id = 1)\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   guid: train-4\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   tokens: [CLS] sa ##git ##tal and axial , and flair t ##2 and su ##sc ##ept ##ibility as well as diffusion axial images of the brain were obtained . following ga ##do ##lini ##um t ##1 sa ##git ##tal axial and corona ##l images were obtained . comparison was made with the previous mri study of . findings : the diffusion images demonstrate areas of restricted diffusion in the left par ##ie ##tal region slightly posterior to the area of surgical defect . this is suspicious for acute in ##far ##ct in the left par ##ie ##tal region . a follow ##up examination is recommended . post operative changes are seen in the left frontal region with blood products . following ga ##do ##lini ##um subtle marginal enhancement is seen at the surgical bed . no evidence of mid ##line shift mass effect or hydro ##ce ##pha ##lus is identified . a small area of blood product in the right posterior temporal region is seen indicating an old area of hem ##or ##rh ##age from trauma or cavern ##ous ang ##iom ##a . impression : probable acute in ##far ##cts in the left par ##ie ##tal region posterior to the cr ##ani ##oto ##my site . two . status post res ##ection of mass in the left frontal region with minimal marginal enhancement at the surgical bed and blood products . three . small area of chronic blood products in the right posterior temporal region could be due to old trauma or a cavern ##ous ang ##iom ##a . correlation with patient ' s prior outside mri studies would be helpful . s / p cr ##ani ##oto ##my - t / sic ##u np ##n 11 ##p - 7 ##a s - ap ##has ##ic o - ne ##uro - see flows ##hee ##t for ne ##uro ass ##es ##ment details . pt given total 2 ##mg at ##iva ##n iv ##p for ag ##git ##ation this am . pt to get o ##ob / sitting up . cv - see flows ##hee ##t for vs data , s ##np gt ##t we ##ane ##d to off this am , dil ##t gt ##t dc ' d @ 6 ##am . res ##p - pt l ##s ct ##a sat ##s ##9 ##5 - ninety - eight % on forty % ft and 4 ##l nc . rr ##16 - twenty - four , na ##rd . gi / gu - pt abd soft + bs , u / o ad ##e ##q am ##ts clear yellow urine via foley , np ##o . skin - head ds ##g cd ##i , pt back + butt ##ock gross ##ly intact . soc - no family contact over . id - af ##eb ##ril ##e end ##o - f ##sb ##s one hundred and forty - nine - one hundred and seventy - three over , ss reg insulin / orders a - alt ne ##uro status s / p cr ##ani ##oto ##my p - con ##t to monitor ne ##uro [SEP]\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   input_ids: 101 7842 23806 9080 1998 26819 1010 1998 22012 1056 2475 1998 10514 11020 23606 13464 2004 2092 2004 19241 26819 4871 1997 1996 4167 2020 4663 1012 2206 11721 3527 22153 2819 1056 2487 7842 23806 9080 26819 1998 21887 2140 4871 2020 4663 1012 7831 2001 2081 2007 1996 3025 27011 2817 1997 1012 9556 1024 1996 19241 4871 10580 2752 1997 7775 19241 1999 1996 2187 11968 2666 9080 2555 3621 15219 2000 1996 2181 1997 11707 21262 1012 2023 2003 10027 2005 11325 1999 14971 6593 1999 1996 2187 11968 2666 9080 2555 1012 1037 3582 6279 7749 2003 6749 1012 2695 12160 3431 2024 2464 1999 1996 2187 19124 2555 2007 2668 3688 1012 2206 11721 3527 22153 2819 11259 14785 22415 2003 2464 2012 1996 11707 2793 1012 2053 3350 1997 3054 4179 5670 3742 3466 2030 18479 3401 21890 7393 2003 4453 1012 1037 2235 2181 1997 2668 4031 1999 1996 2157 15219 15850 2555 2003 2464 8131 2019 2214 2181 1997 19610 2953 25032 4270 2013 12603 2030 16679 3560 17076 18994 2050 1012 8605 1024 15596 11325 1999 14971 16649 1999 1996 2187 11968 2666 9080 2555 15219 2000 1996 13675 7088 11439 8029 2609 1012 2048 1012 3570 2695 24501 18491 1997 3742 1999 1996 2187 19124 2555 2007 10124 14785 22415 2012 1996 11707 2793 1998 2668 3688 1012 2093 1012 2235 2181 1997 11888 2668 3688 1999 1996 2157 15219 15850 2555 2071 2022 2349 2000 2214 12603 2030 1037 16679 3560 17076 18994 2050 1012 16902 2007 5776 1005 1055 3188 2648 27011 2913 2052 2022 14044 1012 1055 1013 1052 13675 7088 11439 8029 1011 1056 1013 14387 2226 27937 2078 2340 2361 1011 1021 2050 1055 1011 9706 14949 2594 1051 1011 11265 10976 1011 2156 6223 21030 2102 2005 11265 10976 4632 2229 3672 4751 1012 13866 2445 2561 1016 24798 2012 11444 2078 4921 2361 2005 12943 23806 3370 2023 2572 1012 13866 2000 2131 1051 16429 1013 3564 2039 1012 26226 1011 2156 6223 21030 2102 2005 5443 2951 1010 1055 16275 14181 2102 2057 7231 2094 2000 2125 2023 2572 1010 29454 2102 14181 2102 5887 1005 1040 1030 1020 3286 1012 24501 2361 1011 13866 1048 2015 14931 2050 2938 2015 2683 2629 1011 13568 1011 2809 1003 2006 5659 1003 3027 1998 1018 2140 13316 1012 25269 16048 1011 3174 1011 2176 1010 6583 4103 1012 21025 1013 19739 1011 13866 19935 3730 1009 18667 1010 1057 1013 1051 4748 2063 4160 2572 3215 3154 3756 17996 3081 17106 1010 27937 2080 1012 3096 1011 2132 16233 2290 3729 2072 1010 13866 2067 1009 10007 7432 7977 2135 10109 1012 27084 1011 2053 2155 3967 2058 1012 8909 1011 21358 15878 15928 2063 2203 2080 1011 1042 19022 2015 2028 3634 1998 5659 1011 3157 1011 2028 3634 1998 10920 1011 2093 2058 1010 7020 19723 22597 1013 4449 1037 1011 12456 11265 10976 3570 1055 1013 1052 13675 7088 11439 8029 1052 1011 9530 2102 2000 8080 11265 10976 102\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/05/2022 12:15:33 - INFO - __main__ -   label: 0 (id = 0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1a0de70ecc1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_features = convert_examples_to_features(\n\u001b[0;32m----> 2\u001b[0;31m             train_examples, label_list, max_seq_length, tokenizer)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-38c892a977e9>\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[0;34m(examples, label_list, max_seq_length, tokenizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mex_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtokens_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtokens_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/anaconda3/lib/python3.6/site-packages/pytorch_pretrained_bert/tokenization.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msub_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/anaconda3/lib/python3.6/site-packages/pytorch_pretrained_bert/tokenization.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_strip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_split_on_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitespace_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/anaconda3/lib/python3.6/site-packages/pytorch_pretrained_bert/tokenization.py\u001b[0m in \u001b[0;36m_run_split_on_punc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_tokenize_chinese_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/anaconda3/lib/python3.6/site-packages/pytorch_pretrained_bert/tokenization.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_tokenize_chinese_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_features = convert_examples_to_features(\n",
    "            train_examples, label_list, max_seq_length, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rank = -1\n",
    "train_batch_size = 56 # testing, reference of Kevin's artical\n",
    "no_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:12:52 - INFO - __main__ -   device cpu n_gpu 0 distributed training False\n",
      "01/05/2022 12:12:53 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/haotiang/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "01/05/2022 12:12:53 - INFO - __main__ -   LOOKING AT /data/users/haotiang/20211217_num2word/3days/train.csv\n"
     ]
    }
   ],
   "source": [
    "processors = {\n",
    "    \"readmission\": readmissionProcessor\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\") # defult setting\n",
    "n_gpu = torch.cuda.device_count() # n_gpu = 0\n",
    "logger.info(\"device %s n_gpu %d distributed training %r\", device, n_gpu, False)\n",
    "gradient_accumulation_steps = 1 # default\n",
    "num_train_epochs = 3 # default, reference of Kevin's artical\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "task_name = 'readmission'\n",
    "processor = processors[task_name]()\n",
    "label_list = processor.get_labels() # ['0', '1']\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_examples = None\n",
    "num_train_steps = None\n",
    "\n",
    "train_examples = processor.get_train_examples('/data/users/haotiang/20211217_num2word/3days')\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in processors:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processors[task_name]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = processors[task_name]()\n",
    "label_list = processor.get_labels()\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Prepare model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:13:14 - INFO - modeling_readmission -   loading archive file /data/users/haotiang/model/pretraining\n",
      "01/05/2022 12:13:14 - INFO - modeling_readmission -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_model = '/data/users/haotiang/model/pretraining'\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model, 1) #load pretrained clinicalBERT model\n",
    "model.to(device)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "# Implements BERT version of Adam algorithm with weight decay fix.\n",
    "learning_rate = 2e-5 # by author\n",
    "warmup_proportion = 0.1 #default\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=learning_rate,\n",
    "                     warmup=warmup_proportion,\n",
    "                     t_total=num_train_steps)\n",
    "\n",
    "global_step = 0\n",
    "train_loss=100000\n",
    "number_training_steps=1\n",
    "global_step_check=0\n",
    "#train_loss_history=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:14:43 - INFO - __main__ -   ***** Running training *****\n",
      "01/05/2022 12:14:43 - INFO - __main__ -     Num examples = 49800\n",
      "01/05/2022 12:14:43 - INFO - __main__ -     Batch size = 56\n",
      "01/05/2022 12:14:43 - INFO - __main__ -     Num steps = 2667\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f0c3a7f557f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  Num steps = %d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mall_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mall_input_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_mask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mall_segment_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "#train_features = convert_examples_to_features(\n",
    "#    train_examples, label_list, args.max_seq_length, tokenizer)\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "if local_rank == -1:\n",
    "    train_sampler = RandomSampler(train_data) # A Sampler that returns random indices\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "# each batch has shape of (4, 56, 512) (only lable_ids has shape 56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(train_features):\n",
    "    if i < 3:\n",
    "        print(type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(train_data):\n",
    "    if i < 3:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(train_sampler):\n",
    "    if i < 10:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_ids, batch_input_mask, batch_segment_ids, batch_label_ids = next(iter(train_dataloader))\n",
    "print(batch_input_ids.size(), batch_input_mask.size(), batch_segment_ids.size(), batch_label_ids.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /data/users/haotiang/model/pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertForSequenceClassification.from_pretrained??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in param_optimizer:\n",
    "    if 'gamma' in n:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertAdam??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default setting\n",
    "num_train_epochs = 3.0\n",
    "fp16 = False\n",
    "loss_scale = 128\n",
    "gradient_accumulation_steps = 1\n",
    "optimize_on_cpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history=[]\n",
    "model.train()\n",
    "# model = BertForSequenceClassification.from_pretrained(bert_model, 1) #load pretrained clinicalBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    if step < 3:\n",
    "        print([t for t in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one epoch\n",
    "train_loss_history=[]\n",
    "tr_loss = 0\n",
    "nb_tr_examples, nb_tr_steps = 0, 0\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    if step < 1: # one batch\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        loss, logits = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "        if fp16 and loss_scale != 1.0:\n",
    "            # rescale loss for fp16 training\n",
    "            # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
    "            loss = loss * loss_scale\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        train_loss_history.append(loss.item())\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # below part is escaped\n",
    "            if fp16 or optimize_on_cpu:\n",
    "                if fp16 and loss_scale != 1.0:\n",
    "                    # scale down gradients for fp16 training\n",
    "                    for param in model.parameters():\n",
    "                        if param.grad is not None:\n",
    "                            param.grad.data = param.grad.data / loss_scale\n",
    "                is_nan = set_optimizer_params_grad(param_optimizer, model.named_parameters(), test_nan=True)\n",
    "                if is_nan:\n",
    "                    logger.info(\"FP16 TRAINING: Nan in gradients, reducing loss scaling\")\n",
    "                    loss_scale = loss_scale / 2\n",
    "                    model.zero_grad()\n",
    "                    continue\n",
    "                optimizer.step()\n",
    "                copy_optimizer_params_to_model(model.named_parameters(), param_optimizer)\n",
    "            else:\n",
    "                # only this is emplementeed\n",
    "                optimizer.step()\n",
    "            model.zero_grad()\n",
    "            global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (step + 1) % gradient_accumulation_steps == 0:\n",
    "    if fp16 or optimize_on_cpu:\n",
    "        if fp16 and loss_scale != 1.0:\n",
    "            # scale down gradients for fp16 training\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(1)\n",
    "        print(2)\n",
    "        #is_nan = set_optimizer_params_grad(param_optimizer, model.named_parameters(), test_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history, tr_loss, nb_tr_examples, nb_tr_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:20:58 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/haotiang/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b) # text_b is not useful in our task\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(max_seq_length - 2)] # only trained with 0: 512-2\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens) \n",
    "        # BertTokenizer.from_pretrained('bert-base-uncased').convert_tokens_to_ids\n",
    "        # convert tokens to number (max 512)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        #print (example.label)\n",
    "        label_id = label_map[example.label]\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_eval = True\n",
    "data_dir = '/data/users/haotiang/20211217_num2word/3days/'\n",
    "eval_batch_size = 2 # default\n",
    "max_seq_length = 512 # from the artical\n",
    "local_rank = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20211217_num2word      data.zip  pretrained_model.zip  result_early\r\n",
      "20211217_num2word.zip  __MACOSX  result_discharge      result_early_finetuned\r\n",
      "data\t\t       model\t result_discharge_num\r\n"
     ]
    }
   ],
   "source": [
    "! ls /data/users/haotiang/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:21:32 - INFO - __main__ -   device cpu n_gpu 0 distributed training False\n",
      "01/05/2022 12:21:32 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/haotiang/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "processors = {\n",
    "    \"readmission\": readmissionProcessor\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\") # defult setting\n",
    "n_gpu = torch.cuda.device_count() # n_gpu = 0\n",
    "logger.info(\"device %s n_gpu %d distributed training %r\", device, n_gpu, False)\n",
    "gradient_accumulation_steps = 1 # default\n",
    "num_train_epochs = 3 # default, reference of Kevin's artical\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "task_name = 'readmission'\n",
    "processor = processors[task_name]()\n",
    "label_list = processor.get_labels() # ['0', '1']\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytorch_pretrained_bert.tokenization.BertTokenizer at 0x7f698c9e7eb8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, device(type='cpu'))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_rank, device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:22:25 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   guid: test-0\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   tokens: [CLS] sin ##us ta ##chy ##card ##ia . generalized low voltage . delayed r wave progression with late pre ##cor ##dial q ##rs transition . findings are non - specific . clinical correlation is suggested . since the previous tracing of same date sin ##us ta ##chy ##card ##ia rate is faster . tracing # one sin ##us rhythm . delayed r wave progression with late pre ##cor ##dial q ##rs transition . generalized low q ##rs voltage . findings are non - specific . clinical correlation is suggested . since the previous tracing of the rate is faster and voltage is lower . title : forty - five y / o man with h ##x of et ##oh ci ##rr ##hosis who arrived for planned liver transplant . went to or am and received pt from or approx one thousand , eight hundred int ##uba ##ted and se ##date ##d on prop ##of ##ol . transplant , liver assessment : int ##uba ##ted on ac at forty % fi ##o ##2 . lungs clear to diminished in bases . ng ##t to lc ##ws . absent bow ##el sounds . un ##res ##pon ##sive . not withdrawing to painful stimuli . pupils slug ##gis ##h at 3 ##mm . se ##date ##d on prop ##of ##ol . cc ##o initiated . swan - gan ##z cat ##het ##er intact . jp drains x ##2 ( medial / lateral ) to self su ##ction . abdominal inc ##ision with small amount ser ##o - sang drainage . action : all post - op labs sent . vi ##gil ##eo cal ##ib ##rated . ek ##g obtained . c ##x ##r obtained . iv ##f at 125 ##cc / hr . response : no vent changes . su ##ction ##ing for scan ##t secret ##ions , small amount bloody oral secret ##ions . ng ##t with scan ##t amounts of bright red blood output . no changes in ne ##uro exam . jp drains with scan ##t amount of ser ##o - sang drainage . plan : continue to monitor ne ##uro exam every four hours . monitor hc ##t every two hours with ab ##gs and labs every four hours . ( goal hc ##t > thirty ) trans ##fus ##e pt according to goals . continue to treat pt per post - op orders and transplant chart . title : received pt from or approx one thousand , eight hundred int ##uba ##ted and se ##date ##d . un ##res ##pon ##sive on exam not withdrawing or local ##izing . five : nineteen am chest ( pre - op pa & la ##t ) clip # reason : pre op film medical condition : forty - five year old man with liver ci ##rr ##hosis reason for this examination : pre op film wet read : ip ##f tu ##e five : fifty - three am no pneumonia . small left pl ##eur ##al e ##ff ##usion . stable vascular redistribution . final report history : a forty - five [SEP]\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   input_ids: 101 8254 2271 11937 11714 11522 2401 1012 18960 2659 10004 1012 8394 1054 4400 14967 2007 2397 3653 27108 27184 1053 2869 6653 1012 9556 2024 2512 1011 3563 1012 6612 16902 2003 4081 1012 2144 1996 3025 16907 1997 2168 3058 8254 2271 11937 11714 11522 2401 3446 2003 5514 1012 16907 1001 2028 8254 2271 6348 1012 8394 1054 4400 14967 2007 2397 3653 27108 27184 1053 2869 6653 1012 18960 2659 1053 2869 10004 1012 9556 2024 2512 1011 3563 1012 6612 16902 2003 4081 1012 2144 1996 3025 16907 1997 1996 3446 2003 5514 1998 10004 2003 2896 1012 2516 1024 5659 1011 2274 1061 1013 1051 2158 2007 1044 2595 1997 3802 11631 25022 12171 25229 2040 3369 2005 3740 11290 22291 1012 2253 2000 2030 2572 1998 2363 13866 2013 2030 22480 2028 4595 1010 2809 3634 20014 19761 3064 1998 7367 13701 2094 2006 17678 11253 4747 1012 22291 1010 11290 7667 1024 20014 19761 3064 2006 9353 2012 5659 1003 10882 2080 2475 1012 8948 3154 2000 15911 1999 7888 1012 12835 2102 2000 29215 9333 1012 9962 6812 2884 4165 1012 4895 6072 26029 12742 1012 2025 21779 2000 9145 22239 1012 7391 23667 17701 2232 2012 1017 7382 1012 7367 13701 2094 2006 17678 11253 4747 1012 10507 2080 7531 1012 10677 1011 25957 2480 4937 27065 2121 10109 1012 16545 18916 1060 2475 1006 23828 1013 11457 1007 2000 2969 10514 7542 1012 21419 4297 19969 2007 2235 3815 14262 2080 1011 6369 11987 1012 2895 1024 2035 2695 1011 6728 13625 2741 1012 6819 20142 8780 10250 12322 9250 1012 23969 2290 4663 1012 1039 2595 2099 4663 1012 4921 2546 2012 8732 9468 1013 17850 1012 3433 1024 2053 18834 3431 1012 10514 7542 2075 2005 13594 2102 3595 8496 1010 2235 3815 6703 8700 3595 8496 1012 12835 2102 2007 13594 2102 8310 1997 4408 2417 2668 6434 1012 2053 3431 1999 11265 10976 11360 1012 16545 18916 2007 13594 2102 3815 1997 14262 2080 1011 6369 11987 1012 2933 1024 3613 2000 8080 11265 10976 11360 2296 2176 2847 1012 8080 16731 2102 2296 2048 2847 2007 11113 5620 1998 13625 2296 2176 2847 1012 1006 3125 16731 2102 1028 4228 1007 9099 25608 2063 13866 2429 2000 3289 1012 3613 2000 7438 13866 2566 2695 1011 6728 4449 1998 22291 3673 1012 2516 1024 2363 13866 2013 2030 22480 2028 4595 1010 2809 3634 20014 19761 3064 1998 7367 13701 2094 1012 4895 6072 26029 12742 2006 11360 2025 21779 2030 2334 6026 1012 2274 1024 11977 2572 3108 1006 3653 1011 6728 6643 1004 2474 2102 1007 12528 1001 3114 1024 3653 6728 2143 2966 4650 1024 5659 1011 2274 2095 2214 2158 2007 11290 25022 12171 25229 3114 2005 2023 7749 1024 3653 6728 2143 4954 3191 1024 12997 2546 10722 2063 2274 1024 5595 1011 2093 2572 2053 18583 1012 2235 2187 20228 11236 2389 1041 4246 14499 1012 6540 21449 25707 1012 2345 3189 2381 1024 1037 5659 1011 2274 102\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   label: 1 (id = 1)\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   guid: test-1\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   tokens: [CLS] pre ##oper ##ative films for liver transplant . technique : pa and lateral chest radio ##graphs . comparison : compared to radio ##graph from . findings : heart size is moderately enlarged , similar to prior study . there is mild pulmonary vascular redistribution . there is stable small left pl ##eur ##al e ##ff ##usion . interval removal of right pic ##c line . impression : card ##iom ##ega ##ly . no pneumonia . small left pl ##eur ##al e ##ff ##usion . eleven : twenty - six pm chest ( portable ap ) ; - seventy - six by same physician # reason : s / p ol ##t . eva ##l for collapse , over ##load admitting diagnosis : liver tx medical condition : forty - five year old man des ##at ##uration ##s , int ##uba ##ted reason for this examination : s / p ol ##t . eva ##l for collapse , over ##load final report ap chest eleven : thirty - five p . m . , history : des ##at ##uration , after liver transplant . impression : ap chest compared to six : forty p . m . : widespread pulmonary op ##ac ##ification most likely ed ##ema , changed only minimal ##ly since six : forty p . m . mild card ##iom ##ega ##ly stable . et tube in standard placement . swan - gan ##z cat ##het ##er ends in the right pulmonary artery . no p ##ne ##um ##otho ##ra ##x . lateral aspect of left lower chest is excluded from the examination . six : twelve pm chest port . line placement ; - fifty - nine distinct procedural service clip # reason : line , tube position . ? pt ##x admitting diagnosis : liver tx medical condition : forty - five year old man s / p liver transplant reason for this examination : line , tube position . ? pt ##x wet read : ag ##lc wed twelve : thirteen am et ##t five . four cm above car ##ina . lt i ##j swan tip likely in pu ##lm out ##flow tract . og ##t into stomach and out of view inferior ##ly . ru ##q drain . low lung volumes . moderate card ##iom ##ega ##ly as before , now with patch ##y upper lobe and per ##ih ##ila ##r op ##ac ##ities , probably due to pulmonary ed ##ema . lt retro ##card ##iac op ##ac ##ity likely pl ##eur ##al e ##ff ##usion with ate ##le ##cta ##sis . no su ##pine evidence of large pt ##x . final report ap chest six : forty p . m . on history : liver transplant , check line and tube positions and p ##ne ##um ##otho ##ra ##x . impression : ap chest compared to , five : nineteen a . m . : widespread het ##ero ##gen ##eous op ##ac ##ification is developed in both lungs , with large areas of ground - glass radio ##den ##sity and / or [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:22:25 - INFO - __main__ -   input_ids: 101 3653 25918 8082 3152 2005 11290 22291 1012 6028 1024 6643 1998 11457 3108 2557 27341 1012 7831 1024 4102 2000 2557 14413 2013 1012 9556 1024 2540 2946 2003 17844 11792 1010 2714 2000 3188 2817 1012 2045 2003 10256 21908 21449 25707 1012 2045 2003 6540 2235 2187 20228 11236 2389 1041 4246 14499 1012 13483 8208 1997 2157 27263 2278 2240 1012 8605 1024 4003 18994 29107 2135 1012 2053 18583 1012 2235 2187 20228 11236 2389 1041 4246 14499 1012 5408 1024 3174 1011 2416 7610 3108 1006 12109 9706 1007 1025 1011 10920 1011 2416 2011 2168 7522 1001 3114 1024 1055 1013 1052 19330 2102 1012 9345 2140 2005 7859 1010 2058 11066 17927 11616 1024 11290 19067 2966 4650 1024 5659 1011 2274 2095 2214 2158 4078 4017 18924 2015 1010 20014 19761 3064 3114 2005 2023 7749 1024 1055 1013 1052 19330 2102 1012 9345 2140 2005 7859 1010 2058 11066 2345 3189 9706 3108 5408 1024 4228 1011 2274 1052 1012 1049 1012 1010 2381 1024 4078 4017 18924 1010 2044 11290 22291 1012 8605 1024 9706 3108 4102 2000 2416 1024 5659 1052 1012 1049 1012 1024 6923 21908 6728 6305 9031 2087 3497 3968 14545 1010 2904 2069 10124 2135 2144 2416 1024 5659 1052 1012 1049 1012 10256 4003 18994 29107 2135 6540 1012 3802 7270 1999 3115 11073 1012 10677 1011 25957 2480 4937 27065 2121 4515 1999 1996 2157 21908 16749 1012 2053 1052 2638 2819 29288 2527 2595 1012 11457 7814 1997 2187 2896 3108 2003 12421 2013 1996 7749 1012 2416 1024 4376 7610 3108 3417 1012 2240 11073 1025 1011 5595 1011 3157 5664 24508 2326 12528 1001 3114 1024 2240 1010 7270 2597 1012 1029 13866 2595 17927 11616 1024 11290 19067 2966 4650 1024 5659 1011 2274 2095 2214 2158 1055 1013 1052 11290 22291 3114 2005 2023 7749 1024 2240 1010 7270 2597 1012 1029 13866 2595 4954 3191 1024 12943 15472 21981 4376 1024 7093 2572 3802 2102 2274 1012 2176 4642 2682 2482 3981 1012 8318 1045 3501 10677 5955 3497 1999 16405 13728 2041 12314 12859 1012 13958 2102 2046 4308 1998 2041 1997 3193 14092 2135 1012 21766 4160 12475 1012 2659 11192 6702 1012 8777 4003 18994 29107 2135 2004 2077 1010 2085 2007 8983 2100 3356 21833 1998 2566 19190 11733 2099 6728 6305 6447 1010 2763 2349 2000 21908 3968 14545 1012 8318 22307 11522 20469 6728 6305 3012 3497 20228 11236 2389 1041 4246 14499 2007 8823 2571 25572 6190 1012 2053 10514 19265 3350 1997 2312 13866 2595 1012 2345 3189 9706 3108 2416 1024 5659 1052 1012 1049 1012 2006 2381 1024 11290 22291 1010 4638 2240 1998 7270 4460 1998 1052 2638 2819 29288 2527 2595 1012 8605 1024 9706 3108 4102 2000 1010 2274 1024 11977 1037 1012 1049 1012 1024 6923 21770 10624 6914 14769 6728 6305 9031 2003 2764 1999 2119 8948 1010 2007 2312 2752 1997 2598 1011 3221 2557 4181 17759 1998 1013 2030 102\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   label: 1 (id = 1)\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   guid: test-2\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   tokens: [CLS] tip of the left trans ##ju ##gul ##ar swan - gan ##z cat ##het ##er projects over the right vent ##ric ##le below the out ##flow tract . no media ##sti ##nal widening or app ##re ##cia ##ble pl ##eur ##al e ##ff ##usion . no p ##ne ##um ##otho ##ra ##x . et tube is at the thor ##ac ##ic inlet . sin ##us rhythm with border ##line sin ##us ta ##chy ##card ##ia . generalized low voltage . delayed r wave progression with late pre ##cor ##dial q ##rs transition . findings are non - specific . clinical correlation is suggested . since the previous tracing of sin ##us ta ##chy ##card ##ia rate is slower and right pre ##cor ##dial lead t wave changes appear more prominent . tracing # two renal failure , acute ( acute renal failure , ar ##f ) assessment : pt with low u ##o from or making only four hundred and thirty - five for entire length of case h / o of at ##n / ar ##f ict ##eric , clear urine , some sediment hu ##o zero - 80 ##cc / h action : intermittent bo ##lus ##es of fur ##ose ##mide in between and following pr ##bc ##s hu ##o monitored and documented transplant / sic ##u teams aware of hu ##o or lack there of pt made kv ##o d / t fl ##d intake in products from or and here in sic ##u labs q four hr response : no response from fur ##ose ##mide labs showing cr taking bump from one . zero to one . eight plan : continue to monitor at ##n continue to monitor cr / bun / renal function respiratory failure , acute ( not ar ##ds / ) assessment : pt initially on ac 850 ##x ##14 , forty % from or an ##esthesia settings ab ##gs getting more al ##kal ##otic post - fur ##ose ##mide doses o ##2 sat ##s ~ ninety - five % - ninety - seven % briefly dropping o ##2 sat ##s ~ ninety and not rebound ##ing with su ##ction action : ab ##gs drawn q ##2 rt in to evaluate pt with drop in sat , rebound ##ing spontaneously after several inter ##vet ##ions attempted ab ##gs also reflecting a drop in pa ##0 ##2 tv lowered to eight hundred as pt growing more al ##kal ##otic response : ab ##gs trend ##ing al ##kal ##otic plan : ab ##gs followed q ##2 hr f / u at six hundred for ab ##g on adjusted vent settings renal failure , acute ( acute renal failure , ar ##f ) assessment : pt with low u ##o from or making only four hundred and thirty - five for entire length of case h / o of at ##n / ar ##f ict ##eric , clear urine , some sediment hu ##o zero - 80 ##cc / h action : intermittent bo ##lus ##es of fur ##ose ##mide in between and following pr ##bc [SEP]\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   input_ids: 101 5955 1997 1996 2187 9099 9103 24848 2906 10677 1011 25957 2480 4937 27065 2121 3934 2058 1996 2157 18834 7277 2571 2917 1996 2041 12314 12859 1012 2053 2865 16643 12032 17973 2030 10439 2890 7405 3468 20228 11236 2389 1041 4246 14499 1012 2053 1052 2638 2819 29288 2527 2595 1012 3802 7270 2003 2012 1996 15321 6305 2594 15824 1012 8254 2271 6348 2007 3675 4179 8254 2271 11937 11714 11522 2401 1012 18960 2659 10004 1012 8394 1054 4400 14967 2007 2397 3653 27108 27184 1053 2869 6653 1012 9556 2024 2512 1011 3563 1012 6612 16902 2003 4081 1012 2144 1996 3025 16907 1997 8254 2271 11937 11714 11522 2401 3446 2003 12430 1998 2157 3653 27108 27184 2599 1056 4400 3431 3711 2062 4069 1012 16907 1001 2048 25125 4945 1010 11325 1006 11325 25125 4945 1010 12098 2546 1007 7667 1024 13866 2007 2659 1057 2080 2013 2030 2437 2069 2176 3634 1998 4228 1011 2274 2005 2972 3091 1997 2553 1044 1013 1051 1997 2012 2078 1013 12098 2546 25891 22420 1010 3154 17996 1010 2070 19671 15876 2080 5717 1011 3770 9468 1013 1044 2895 1024 23852 8945 7393 2229 1997 6519 9232 24284 1999 2090 1998 2206 10975 9818 2015 15876 2080 17785 1998 8832 22291 1013 14387 2226 2780 5204 1997 15876 2080 2030 3768 2045 1997 13866 2081 24888 2080 1040 1013 1056 13109 2094 13822 1999 3688 2013 2030 1998 2182 1999 14387 2226 13625 1053 2176 17850 3433 1024 2053 3433 2013 6519 9232 24284 13625 4760 13675 2635 16906 2013 2028 1012 5717 2000 2028 1012 2809 2933 1024 3613 2000 8080 2012 2078 3613 2000 8080 13675 1013 21122 1013 25125 3853 16464 4945 1010 11325 1006 2025 12098 5104 1013 1007 7667 1024 13866 3322 2006 9353 15678 2595 16932 1010 5659 1003 2013 2030 2019 25344 10906 11113 5620 2893 2062 2632 12902 20214 2695 1011 6519 9232 24284 21656 1051 2475 2938 2015 1066 13568 1011 2274 1003 1011 13568 1011 2698 1003 4780 7510 1051 2475 2938 2015 1066 13568 1998 2025 27755 2075 2007 10514 7542 2895 1024 11113 5620 4567 1053 2475 19387 1999 2000 16157 13866 2007 4530 1999 2938 1010 27755 2075 27491 2044 2195 6970 19510 8496 4692 11113 5620 2036 10842 1037 4530 1999 6643 2692 2475 2694 6668 2000 2809 3634 2004 13866 3652 2062 2632 12902 20214 3433 1024 11113 5620 9874 2075 2632 12902 20214 2933 1024 11113 5620 2628 1053 2475 17850 1042 1013 1057 2012 2416 3634 2005 11113 2290 2006 10426 18834 10906 25125 4945 1010 11325 1006 11325 25125 4945 1010 12098 2546 1007 7667 1024 13866 2007 2659 1057 2080 2013 2030 2437 2069 2176 3634 1998 4228 1011 2274 2005 2972 3091 1997 2553 1044 1013 1051 1997 2012 2078 1013 12098 2546 25891 22420 1010 3154 17996 1010 2070 19671 15876 2080 5717 1011 3770 9468 1013 1044 2895 1024 23852 8945 7393 2229 1997 6519 9232 24284 1999 2090 1998 2206 10975 9818 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:22:25 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   label: 1 (id = 1)\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   guid: test-3\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   tokens: [CLS] transplant / sic ##u teams aware of hu ##o or lack there of pt made kv ##o d / t fl ##d intake in products from or and here in sic ##u labs q four hr response : no response from fur ##ose ##mide labs showing cr taking bump from one . zero to one . eight plan : continue to monitor at ##n continue to monitor cr / bun / renal function respiratory failure , acute ( not ar ##ds / ) assessment : pt initially on ac 850 ##x ##14 , forty % from or an ##esthesia settings ab ##gs getting more al ##kal ##otic post - fur ##ose ##mide doses o ##2 sat ##s ~ ninety - five % - ninety - seven % briefly dropping o ##2 sat ##s ~ ninety and not rebound ##ing with su ##ction action : ab ##gs drawn q ##2 rt in to evaluate pt with drop in sat , rebound ##ing spontaneously after several inter ##vet ##ions attempted su ##ction ##ing , c ##x ##r , re - positioning ab ##gs also reflecting a drop in pa ##0 ##2 tv lowered to eight hundred as pt growing more al ##kal ##otic response : ab ##gs trend ##ing al ##kal ##otic plan : ab ##gs followed q ##2 hr f / u at six hundred for ab ##g on adjusted vent settings transplant , liver assessment : pt received @ one thousand , nine hundred shortly after arriving in sic ##u from or post - liver transplant ng ##t to lc ##ws for bright to dark red bloody output na ##res packed as pt was o ##oz ##ing blood from back of mouth / na ##res in or absent bow ##el sounds inc ##ision with initial ds ##g intact , re - info ##rce ##d several times throughout shift cop ##ious s / s d ##ng se ##date ##d on prop ##of ##ol not withdrawing to painful stimuli , but facial grim ##acing , able to move head / neck to stimuli , ex ##tre ##mit ##ies very ed ##ema ##tou ##s / heavy pupils slug ##gis ##h at 3 ##mm cc ##o initiated at change of shift , swan - gan ##z cat ##het ##er intact , fifty - five cm pt hyper ##dy ##nami ##c elevated pa numbers , hyper ##tens ##ive to sb ##p 160 ##s , ta ##chy ##card ##ic to 120 ##s jp drains x ##2 ( medial / lateral ) to self su ##ction putting out ~ five - 50 ##cc / hr action : c ##x ##r obtained iv ##f at 125 ##cc / hr changed to kv ##o as pt receiving l ##g am ##ts of fl ##d in products labs q 4 ##hr , ab ##gs q ##2 ##hr drains emptied q one hr ab ##x , cell ##ce ##pt as ordered pr ##bc ##s x ##3 units for goal hc ##t > thirty intermittent bo ##lus ##es fur ##ose ##mide in between / after response : hc ##t ~ twenty - [SEP]\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   input_ids: 101 22291 1013 14387 2226 2780 5204 1997 15876 2080 2030 3768 2045 1997 13866 2081 24888 2080 1040 1013 1056 13109 2094 13822 1999 3688 2013 2030 1998 2182 1999 14387 2226 13625 1053 2176 17850 3433 1024 2053 3433 2013 6519 9232 24284 13625 4760 13675 2635 16906 2013 2028 1012 5717 2000 2028 1012 2809 2933 1024 3613 2000 8080 2012 2078 3613 2000 8080 13675 1013 21122 1013 25125 3853 16464 4945 1010 11325 1006 2025 12098 5104 1013 1007 7667 1024 13866 3322 2006 9353 15678 2595 16932 1010 5659 1003 2013 2030 2019 25344 10906 11113 5620 2893 2062 2632 12902 20214 2695 1011 6519 9232 24284 21656 1051 2475 2938 2015 1066 13568 1011 2274 1003 1011 13568 1011 2698 1003 4780 7510 1051 2475 2938 2015 1066 13568 1998 2025 27755 2075 2007 10514 7542 2895 1024 11113 5620 4567 1053 2475 19387 1999 2000 16157 13866 2007 4530 1999 2938 1010 27755 2075 27491 2044 2195 6970 19510 8496 4692 10514 7542 2075 1010 1039 2595 2099 1010 2128 1011 19120 11113 5620 2036 10842 1037 4530 1999 6643 2692 2475 2694 6668 2000 2809 3634 2004 13866 3652 2062 2632 12902 20214 3433 1024 11113 5620 9874 2075 2632 12902 20214 2933 1024 11113 5620 2628 1053 2475 17850 1042 1013 1057 2012 2416 3634 2005 11113 2290 2006 10426 18834 10906 22291 1010 11290 7667 1024 13866 2363 1030 2028 4595 1010 3157 3634 3859 2044 7194 1999 14387 2226 2013 2030 2695 1011 11290 22291 12835 2102 2000 29215 9333 2005 4408 2000 2601 2417 6703 6434 6583 6072 8966 2004 13866 2001 1051 18153 2075 2668 2013 2067 1997 2677 1013 6583 6072 1999 2030 9962 6812 2884 4165 4297 19969 2007 3988 16233 2290 10109 1010 2128 1011 18558 19170 2094 2195 2335 2802 5670 8872 6313 1055 1013 1055 1040 3070 7367 13701 2094 2006 17678 11253 4747 2025 21779 2000 9145 22239 1010 2021 13268 11844 26217 1010 2583 2000 2693 2132 1013 3300 2000 22239 1010 4654 7913 22930 3111 2200 3968 14545 24826 2015 1013 3082 7391 23667 17701 2232 2012 1017 7382 10507 2080 7531 2012 2689 1997 5670 1010 10677 1011 25957 2480 4937 27065 2121 10109 1010 5595 1011 2274 4642 13866 23760 5149 28987 2278 8319 6643 3616 1010 23760 25808 3512 2000 24829 2361 8148 2015 1010 11937 11714 11522 2594 2000 6036 2015 16545 18916 1060 2475 1006 23828 1013 11457 1007 2000 2969 10514 7542 5128 2041 1066 2274 1011 2753 9468 1013 17850 2895 1024 1039 2595 2099 4663 4921 2546 2012 8732 9468 1013 17850 2904 2000 24888 2080 2004 13866 4909 1048 2290 2572 3215 1997 13109 2094 1999 3688 13625 1053 1018 8093 1010 11113 5620 1053 2475 8093 18916 21764 1053 2028 17850 11113 2595 1010 3526 3401 13876 2004 3641 10975 9818 2015 1060 2509 3197 2005 3125 16731 2102 1028 4228 23852 8945 7393 2229 6519 9232 24284 1999 2090 1013 2044 3433 1024 16731 2102 1066 3174 1011 102\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:22:25 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   label: 1 (id = 1)\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   guid: test-4\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   tokens: [CLS] hc ##t twenty - six . two % with four hundred and thirty labs plan : monitor hc ##t every two hours with ab ##gs and labs every four hours . ( goal hc ##t > thirty ) trans ##fus ##e pt according to goals ? additional pr ##bc ##s to treat hc ##t ~ twenty - six - twenty - seven for goal > thirty continue pathway sic ##u hp ##i : 45 ##yo m w / et ##oh ci ##rr ##hosis , mel ##d twenty - eight , child class c ci ##rr ##hosis admitted for liver transplant . denies change in health since previous admission eight / . admitted to for fever ##s , an ##emia , as ##cite ##s , ar ##f . hospital course , treated w / z ##os ##yn for c . per ##fr ##ingen ##s ba ##cter ##emia . para ##cent ##esis negative for sb ##p . e ##g ##d w / grade i var ##ices . ar ##f responded to oct ##re ##otide , mid ##od ##rine . chief complaint : liver failure pm ##h ##x : pm ##h : ( ) : et ##oh ci ##rr ##hosis , go ##ut , h ##t ##n now norm ##ote ##ns ##ive off med ##s , gr one es ##op ##ha ##ge ##al var ##ices , c per ##fr ##ingen ##s ba ##cter ##emia . . ps ##h : remote app ##end ##ect ##omy . : f ##olic acid one , th ##iam ##ine hc ##l one hundred , ur ##so ##dio ##l three hundred ' ' ' , rani ##ti ##dine hc ##l one hundred and fifty ' ' , lac ##tu ##los ##e , fur ##ose ##mide twenty , sp ##iro ##no ##la ##cton ##e one hundred , z ##of ##ran four , ma ##alo ##x , ri ##fa ##xi ##min two hundred ' ' ' current medications : fifteen . mor ##phine sulfate one - four mg iv q ##1 ##h : pr ##n pain order date : @ one thousand , seven hundred and fifteen sixteen . my ##co ##ph ##eno ##late mo ##fe ##ti ##l suspension one thousand mg po / ng order date : @ one thousand , seven hundred and fifteen three . amp ##ici ##llin - sul ##ba ##cta ##m three gm iv q ##6 ##h duration : three doses order date : @ one thousand , seven hundred and fifteen seventeen . on ##dan ##set ##ron four mg iv q ##8 ##h : pr ##n nausea / vomiting order date : @ one thousand , seven hundred and fifteen four . ch ##lor ##he ##xi ##dine g ##lu ##cona ##te zero . twelve % oral ri ##nse fifteen ml oral use only if patient is on mechanical ventilation . order date : @ one thousand , seven hundred and fifty - eight eighteen . pan ##top ##raz ##ole forty mg iv q ##24 ##h order date : @ one thousand , seven hundred and fifteen five . doc ##usa ##te sodium one hundred mg [SEP]\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   input_ids: 101 16731 2102 3174 1011 2416 1012 2048 1003 2007 2176 3634 1998 4228 13625 2933 1024 8080 16731 2102 2296 2048 2847 2007 11113 5620 1998 13625 2296 2176 2847 1012 1006 3125 16731 2102 1028 4228 1007 9099 25608 2063 13866 2429 2000 3289 1029 3176 10975 9818 2015 2000 7438 16731 2102 1066 3174 1011 2416 1011 3174 1011 2698 2005 3125 1028 4228 3613 12732 14387 2226 6522 2072 1024 3429 7677 1049 1059 1013 3802 11631 25022 12171 25229 1010 11463 2094 3174 1011 2809 1010 2775 2465 1039 25022 12171 25229 4914 2005 11290 22291 1012 23439 2689 1999 2740 2144 3025 9634 2809 1013 1012 4914 2000 2005 9016 2015 1010 2019 17577 1010 2004 17847 2015 1010 12098 2546 1012 2902 2607 1010 5845 1059 1013 1062 2891 6038 2005 1039 1012 2566 19699 15542 2015 8670 21162 17577 1012 11498 13013 19009 4997 2005 24829 2361 1012 1041 2290 2094 1059 1013 3694 1045 13075 23522 1012 12098 2546 5838 2000 13323 2890 26601 1010 3054 7716 11467 1012 2708 12087 1024 11290 4945 7610 2232 2595 1024 7610 2232 1024 1006 1007 1024 3802 11631 25022 12171 25229 1010 2175 4904 1010 1044 2102 2078 2085 13373 12184 3619 3512 2125 19960 2015 1010 24665 2028 9686 7361 3270 3351 2389 13075 23522 1010 1039 2566 19699 15542 2015 8670 21162 17577 1012 1012 8827 2232 1024 6556 10439 10497 22471 16940 1012 1024 1042 23518 5648 2028 1010 16215 25107 3170 16731 2140 2028 3634 1010 24471 6499 20617 2140 2093 3634 1005 1005 1005 1010 21617 3775 10672 16731 2140 2028 3634 1998 5595 1005 1005 1010 18749 8525 10483 2063 1010 6519 9232 24284 3174 1010 11867 9711 3630 2721 28312 2063 2028 3634 1010 1062 11253 5521 2176 1010 5003 23067 2595 1010 15544 7011 9048 10020 2048 3634 1005 1005 1005 2783 20992 1024 5417 1012 22822 20738 26754 2028 1011 2176 11460 4921 1053 2487 2232 1024 10975 2078 3255 2344 3058 1024 1030 2028 4595 1010 2698 3634 1998 5417 7032 1012 2026 3597 8458 16515 13806 9587 7959 3775 2140 8636 2028 4595 11460 13433 1013 12835 2344 3058 1024 1030 2028 4595 1010 2698 3634 1998 5417 2093 1012 23713 28775 21202 1011 21396 3676 25572 2213 2093 13938 4921 1053 2575 2232 9367 1024 2093 21656 2344 3058 1024 1030 2028 4595 1010 2698 3634 1998 5417 9171 1012 2006 7847 13462 4948 2176 11460 4921 1053 2620 2232 1024 10975 2078 19029 1013 24780 2344 3058 1024 1030 2028 4595 1010 2698 3634 1998 5417 2176 1012 10381 10626 5369 9048 10672 1043 7630 24366 2618 5717 1012 4376 1003 8700 15544 12325 5417 19875 8700 2224 2069 2065 5776 2003 2006 6228 19536 1012 2344 3058 1024 1030 2028 4595 1010 2698 3634 1998 5595 1011 2809 7763 1012 6090 14399 20409 9890 5659 11460 4921 1053 18827 2232 2344 3058 1024 1030 2028 4595 1010 2698 3634 1998 5417 2274 1012 9986 10383 2618 13365 2028 3634 11460 102\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "01/05/2022 12:22:25 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:22:25 - INFO - __main__ -   label: 1 (id = 1)\n",
      "01/05/2022 12:22:55 - INFO - __main__ -   ***** Running evaluation *****\n",
      "01/05/2022 12:22:55 - INFO - __main__ -     Num examples = 5687\n",
      "01/05/2022 12:22:55 - INFO - __main__ -     Batch size = 2\n"
     ]
    }
   ],
   "source": [
    "m = nn.Sigmoid()\n",
    "if do_eval:\n",
    "    eval_examples = processor.get_test_examples(data_dir)\n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    if local_rank == -1:\n",
    "        eval_sampler = SequentialSampler(eval_data) # shuffle the test data\n",
    "    else:\n",
    "        eval_sampler = DistributedSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = processor.get_test_examples(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5687"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>sinus tachycardia. generalized low voltage. de...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>537</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>preoperative films for liver transplant. techn...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>538</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>tip of the left transjugular swan-ganz cathete...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>539</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>transplant/sicu teams aware of huo or lack the...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>hct twenty-six.two% with four hundred and thir...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>401376</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>for obstruction. nursing progress note full co...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5683</th>\n",
       "      <td>401377</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>also displayed. ct of the chest with and witho...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5684</th>\n",
       "      <td>401378</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>abdomen. ct of the pelvis with and without iv ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5685</th>\n",
       "      <td>401379</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>totally drained 3800cc. later around two thous...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5686</th>\n",
       "      <td>401380</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>stable post-op. no further bleeding w/ stable ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5687 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        ID                                               TEXT  \\\n",
       "0            536  100133.0  sinus tachycardia. generalized low voltage. de...   \n",
       "1            537  100133.0  preoperative films for liver transplant. techn...   \n",
       "2            538  100133.0  tip of the left transjugular swan-ganz cathete...   \n",
       "3            539  100133.0  transplant/sicu teams aware of huo or lack the...   \n",
       "4            540  100133.0  hct twenty-six.two% with four hundred and thir...   \n",
       "...          ...       ...                                                ...   \n",
       "5682      401376  199930.0  for obstruction. nursing progress note full co...   \n",
       "5683      401377  199930.0  also displayed. ct of the chest with and witho...   \n",
       "5684      401378  199930.0  abdomen. ct of the pelvis with and without iv ...   \n",
       "5685      401379  199930.0  totally drained 3800cc. later around two thous...   \n",
       "5686      401380  199930.0  stable post-op. no further bleeding w/ stable ...   \n",
       "\n",
       "      Label  \n",
       "0       1.0  \n",
       "1       1.0  \n",
       "2       1.0  \n",
       "3       1.0  \n",
       "4       1.0  \n",
       "...     ...  \n",
       "5682    0.0  \n",
       "5683    0.0  \n",
       "5684    0.0  \n",
       "5685    0.0  \n",
       "5686    0.0  \n",
       "\n",
       "[5687 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_2 = pd.read_csv('/data/users/haotiang/20211217_num2word/3days/test.csv')\n",
    "#pd_2['len'] = pd_2[].map(lambda x: len(x))\n",
    "#pd_1.sort_values(by=['len'], ascending=False)\n",
    "pd_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "test_eval = []\n",
    "for (ex_index, example) in enumerate(eval_examples):\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    test_eval.append((example.guid, tokens_a, tokens_b))\n",
    "    \n",
    "pd_3 = pd.DataFrame(test_eval)\n",
    "pd_3['len'] = pd_3[1].map(lambda x: len(x))\n",
    "#pd_1.sort_values(by=['len'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test-0</td>\n",
       "      <td>[sin, ##us, ta, ##chy, ##card, ##ia, ., genera...</td>\n",
       "      <td>None</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test-1</td>\n",
       "      <td>[pre, ##oper, ##ative, films, for, liver, tran...</td>\n",
       "      <td>None</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test-2</td>\n",
       "      <td>[tip, of, the, left, trans, ##ju, ##gul, ##ar,...</td>\n",
       "      <td>None</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test-3</td>\n",
       "      <td>[transplant, /, sic, ##u, teams, aware, of, hu...</td>\n",
       "      <td>None</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test-4</td>\n",
       "      <td>[hc, ##t, twenty, -, six, ., two, %, with, fou...</td>\n",
       "      <td>None</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>test-5682</td>\n",
       "      <td>[for, obstruction, ., nursing, progress, note,...</td>\n",
       "      <td>None</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5683</th>\n",
       "      <td>test-5683</td>\n",
       "      <td>[also, displayed, ., ct, of, the, chest, with,...</td>\n",
       "      <td>None</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5684</th>\n",
       "      <td>test-5684</td>\n",
       "      <td>[abdomen, ., ct, of, the, pe, ##lvis, with, an...</td>\n",
       "      <td>None</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5685</th>\n",
       "      <td>test-5685</td>\n",
       "      <td>[totally, drained, 380, ##0, ##cc, ., later, a...</td>\n",
       "      <td>None</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5686</th>\n",
       "      <td>test-5686</td>\n",
       "      <td>[stable, post, -, op, ., no, further, bleeding...</td>\n",
       "      <td>None</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5687 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0                                                  1     2  len\n",
       "0        test-0  [sin, ##us, ta, ##chy, ##card, ##ia, ., genera...  None  521\n",
       "1        test-1  [pre, ##oper, ##ative, films, for, liver, tran...  None  544\n",
       "2        test-2  [tip, of, the, left, trans, ##ju, ##gul, ##ar,...  None  516\n",
       "3        test-3  [transplant, /, sic, ##u, teams, aware, of, hu...  None  533\n",
       "4        test-4  [hc, ##t, twenty, -, six, ., two, %, with, fou...  None  591\n",
       "...         ...                                                ...   ...  ...\n",
       "5682  test-5682  [for, obstruction, ., nursing, progress, note,...  None  579\n",
       "5683  test-5683  [also, displayed, ., ct, of, the, chest, with,...  None  554\n",
       "5684  test-5684  [abdomen, ., ct, of, the, pe, ##lvis, with, an...  None  492\n",
       "5685  test-5685  [totally, drained, 380, ##0, ##cc, ., later, a...  None  601\n",
       "5686  test-5686  [stable, post, -, op, ., no, further, bleeding...  None  326\n",
       "\n",
       "[5687 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_3.sort_values(['len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5688 /data/users/haotiang/20211217_num2word/3days/test.csv\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l '/data/users/haotiang/20211217_num2word/3days/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:25:06 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   guid: test-0\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   tokens: [CLS] sin ##us ta ##chy ##card ##ia . generalized low voltage . delayed r wave progression with late pre ##cor ##dial q ##rs transition . findings are non - specific . clinical correlation is suggested . since the previous tracing of same date sin ##us ta ##chy ##card ##ia rate is faster . tracing # one sin ##us rhythm . delayed r wave progression with late pre ##cor ##dial q ##rs transition . generalized low q ##rs voltage . findings are non - specific . clinical correlation is suggested . since the previous tracing of the rate is faster and voltage is lower . title : forty - five y / o man with h ##x of et ##oh ci ##rr ##hosis who arrived for planned liver transplant . went to or am and received pt from or approx one thousand , eight hundred int ##uba ##ted and se ##date ##d on prop ##of ##ol . transplant , liver assessment : int ##uba ##ted on ac at forty % fi ##o ##2 . lungs clear to diminished in bases . ng ##t to lc ##ws . absent bow ##el sounds . un ##res ##pon ##sive . not withdrawing to painful stimuli . pupils slug ##gis ##h at 3 ##mm . se ##date ##d on prop ##of ##ol . cc ##o initiated . swan - gan ##z cat ##het ##er intact . jp drains x ##2 ( medial / lateral ) to self su ##ction . abdominal inc ##ision with small amount ser ##o - sang drainage . action : all post - op labs sent . vi ##gil ##eo cal ##ib ##rated . ek ##g obtained . c ##x ##r obtained . iv ##f at 125 ##cc / hr . response : no vent changes . su ##ction ##ing for scan ##t secret ##ions , small amount bloody oral secret ##ions . ng ##t with scan ##t amounts of bright red blood output . no changes in ne ##uro exam . jp drains with scan ##t amount of ser ##o - sang drainage . plan : continue to monitor ne ##uro exam every four hours . monitor hc ##t every two hours with ab ##gs and labs every four hours . ( goal hc ##t > thirty ) trans ##fus ##e pt according to goals . continue to treat pt per post - op orders and transplant chart . title : received pt from or approx one thousand , eight hundred int ##uba ##ted and se ##date ##d . un ##res ##pon ##sive on exam not withdrawing or local ##izing . five : nineteen am chest ( pre - op pa & la ##t ) clip # reason : pre op film medical condition : forty - five year old man with liver ci ##rr ##hosis reason for this examination : pre op film wet read : ip ##f tu ##e five : fifty - three am no pneumonia . small left pl ##eur ##al e ##ff ##usion . stable vascular redistribution . final report history : a forty - five [SEP]\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   input_ids: 101 8254 2271 11937 11714 11522 2401 1012 18960 2659 10004 1012 8394 1054 4400 14967 2007 2397 3653 27108 27184 1053 2869 6653 1012 9556 2024 2512 1011 3563 1012 6612 16902 2003 4081 1012 2144 1996 3025 16907 1997 2168 3058 8254 2271 11937 11714 11522 2401 3446 2003 5514 1012 16907 1001 2028 8254 2271 6348 1012 8394 1054 4400 14967 2007 2397 3653 27108 27184 1053 2869 6653 1012 18960 2659 1053 2869 10004 1012 9556 2024 2512 1011 3563 1012 6612 16902 2003 4081 1012 2144 1996 3025 16907 1997 1996 3446 2003 5514 1998 10004 2003 2896 1012 2516 1024 5659 1011 2274 1061 1013 1051 2158 2007 1044 2595 1997 3802 11631 25022 12171 25229 2040 3369 2005 3740 11290 22291 1012 2253 2000 2030 2572 1998 2363 13866 2013 2030 22480 2028 4595 1010 2809 3634 20014 19761 3064 1998 7367 13701 2094 2006 17678 11253 4747 1012 22291 1010 11290 7667 1024 20014 19761 3064 2006 9353 2012 5659 1003 10882 2080 2475 1012 8948 3154 2000 15911 1999 7888 1012 12835 2102 2000 29215 9333 1012 9962 6812 2884 4165 1012 4895 6072 26029 12742 1012 2025 21779 2000 9145 22239 1012 7391 23667 17701 2232 2012 1017 7382 1012 7367 13701 2094 2006 17678 11253 4747 1012 10507 2080 7531 1012 10677 1011 25957 2480 4937 27065 2121 10109 1012 16545 18916 1060 2475 1006 23828 1013 11457 1007 2000 2969 10514 7542 1012 21419 4297 19969 2007 2235 3815 14262 2080 1011 6369 11987 1012 2895 1024 2035 2695 1011 6728 13625 2741 1012 6819 20142 8780 10250 12322 9250 1012 23969 2290 4663 1012 1039 2595 2099 4663 1012 4921 2546 2012 8732 9468 1013 17850 1012 3433 1024 2053 18834 3431 1012 10514 7542 2075 2005 13594 2102 3595 8496 1010 2235 3815 6703 8700 3595 8496 1012 12835 2102 2007 13594 2102 8310 1997 4408 2417 2668 6434 1012 2053 3431 1999 11265 10976 11360 1012 16545 18916 2007 13594 2102 3815 1997 14262 2080 1011 6369 11987 1012 2933 1024 3613 2000 8080 11265 10976 11360 2296 2176 2847 1012 8080 16731 2102 2296 2048 2847 2007 11113 5620 1998 13625 2296 2176 2847 1012 1006 3125 16731 2102 1028 4228 1007 9099 25608 2063 13866 2429 2000 3289 1012 3613 2000 7438 13866 2566 2695 1011 6728 4449 1998 22291 3673 1012 2516 1024 2363 13866 2013 2030 22480 2028 4595 1010 2809 3634 20014 19761 3064 1998 7367 13701 2094 1012 4895 6072 26029 12742 2006 11360 2025 21779 2030 2334 6026 1012 2274 1024 11977 2572 3108 1006 3653 1011 6728 6643 1004 2474 2102 1007 12528 1001 3114 1024 3653 6728 2143 2966 4650 1024 5659 1011 2274 2095 2214 2158 2007 11290 25022 12171 25229 3114 2005 2023 7749 1024 3653 6728 2143 4954 3191 1024 12997 2546 10722 2063 2274 1024 5595 1011 2093 2572 2053 18583 1012 2235 2187 20228 11236 2389 1041 4246 14499 1012 6540 21449 25707 1012 2345 3189 2381 1024 1037 5659 1011 2274 102\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   label: 1 (id = 1)\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   guid: test-1\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   tokens: [CLS] pre ##oper ##ative films for liver transplant . technique : pa and lateral chest radio ##graphs . comparison : compared to radio ##graph from . findings : heart size is moderately enlarged , similar to prior study . there is mild pulmonary vascular redistribution . there is stable small left pl ##eur ##al e ##ff ##usion . interval removal of right pic ##c line . impression : card ##iom ##ega ##ly . no pneumonia . small left pl ##eur ##al e ##ff ##usion . eleven : twenty - six pm chest ( portable ap ) ; - seventy - six by same physician # reason : s / p ol ##t . eva ##l for collapse , over ##load admitting diagnosis : liver tx medical condition : forty - five year old man des ##at ##uration ##s , int ##uba ##ted reason for this examination : s / p ol ##t . eva ##l for collapse , over ##load final report ap chest eleven : thirty - five p . m . , history : des ##at ##uration , after liver transplant . impression : ap chest compared to six : forty p . m . : widespread pulmonary op ##ac ##ification most likely ed ##ema , changed only minimal ##ly since six : forty p . m . mild card ##iom ##ega ##ly stable . et tube in standard placement . swan - gan ##z cat ##het ##er ends in the right pulmonary artery . no p ##ne ##um ##otho ##ra ##x . lateral aspect of left lower chest is excluded from the examination . six : twelve pm chest port . line placement ; - fifty - nine distinct procedural service clip # reason : line , tube position . ? pt ##x admitting diagnosis : liver tx medical condition : forty - five year old man s / p liver transplant reason for this examination : line , tube position . ? pt ##x wet read : ag ##lc wed twelve : thirteen am et ##t five . four cm above car ##ina . lt i ##j swan tip likely in pu ##lm out ##flow tract . og ##t into stomach and out of view inferior ##ly . ru ##q drain . low lung volumes . moderate card ##iom ##ega ##ly as before , now with patch ##y upper lobe and per ##ih ##ila ##r op ##ac ##ities , probably due to pulmonary ed ##ema . lt retro ##card ##iac op ##ac ##ity likely pl ##eur ##al e ##ff ##usion with ate ##le ##cta ##sis . no su ##pine evidence of large pt ##x . final report ap chest six : forty p . m . on history : liver transplant , check line and tube positions and p ##ne ##um ##otho ##ra ##x . impression : ap chest compared to , five : nineteen a . m . : widespread het ##ero ##gen ##eous op ##ac ##ification is developed in both lungs , with large areas of ground - glass radio ##den ##sity and / or [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:25:06 - INFO - __main__ -   input_ids: 101 3653 25918 8082 3152 2005 11290 22291 1012 6028 1024 6643 1998 11457 3108 2557 27341 1012 7831 1024 4102 2000 2557 14413 2013 1012 9556 1024 2540 2946 2003 17844 11792 1010 2714 2000 3188 2817 1012 2045 2003 10256 21908 21449 25707 1012 2045 2003 6540 2235 2187 20228 11236 2389 1041 4246 14499 1012 13483 8208 1997 2157 27263 2278 2240 1012 8605 1024 4003 18994 29107 2135 1012 2053 18583 1012 2235 2187 20228 11236 2389 1041 4246 14499 1012 5408 1024 3174 1011 2416 7610 3108 1006 12109 9706 1007 1025 1011 10920 1011 2416 2011 2168 7522 1001 3114 1024 1055 1013 1052 19330 2102 1012 9345 2140 2005 7859 1010 2058 11066 17927 11616 1024 11290 19067 2966 4650 1024 5659 1011 2274 2095 2214 2158 4078 4017 18924 2015 1010 20014 19761 3064 3114 2005 2023 7749 1024 1055 1013 1052 19330 2102 1012 9345 2140 2005 7859 1010 2058 11066 2345 3189 9706 3108 5408 1024 4228 1011 2274 1052 1012 1049 1012 1010 2381 1024 4078 4017 18924 1010 2044 11290 22291 1012 8605 1024 9706 3108 4102 2000 2416 1024 5659 1052 1012 1049 1012 1024 6923 21908 6728 6305 9031 2087 3497 3968 14545 1010 2904 2069 10124 2135 2144 2416 1024 5659 1052 1012 1049 1012 10256 4003 18994 29107 2135 6540 1012 3802 7270 1999 3115 11073 1012 10677 1011 25957 2480 4937 27065 2121 4515 1999 1996 2157 21908 16749 1012 2053 1052 2638 2819 29288 2527 2595 1012 11457 7814 1997 2187 2896 3108 2003 12421 2013 1996 7749 1012 2416 1024 4376 7610 3108 3417 1012 2240 11073 1025 1011 5595 1011 3157 5664 24508 2326 12528 1001 3114 1024 2240 1010 7270 2597 1012 1029 13866 2595 17927 11616 1024 11290 19067 2966 4650 1024 5659 1011 2274 2095 2214 2158 1055 1013 1052 11290 22291 3114 2005 2023 7749 1024 2240 1010 7270 2597 1012 1029 13866 2595 4954 3191 1024 12943 15472 21981 4376 1024 7093 2572 3802 2102 2274 1012 2176 4642 2682 2482 3981 1012 8318 1045 3501 10677 5955 3497 1999 16405 13728 2041 12314 12859 1012 13958 2102 2046 4308 1998 2041 1997 3193 14092 2135 1012 21766 4160 12475 1012 2659 11192 6702 1012 8777 4003 18994 29107 2135 2004 2077 1010 2085 2007 8983 2100 3356 21833 1998 2566 19190 11733 2099 6728 6305 6447 1010 2763 2349 2000 21908 3968 14545 1012 8318 22307 11522 20469 6728 6305 3012 3497 20228 11236 2389 1041 4246 14499 2007 8823 2571 25572 6190 1012 2053 10514 19265 3350 1997 2312 13866 2595 1012 2345 3189 9706 3108 2416 1024 5659 1052 1012 1049 1012 2006 2381 1024 11290 22291 1010 4638 2240 1998 7270 4460 1998 1052 2638 2819 29288 2527 2595 1012 8605 1024 9706 3108 4102 2000 1010 2274 1024 11977 1037 1012 1049 1012 1024 6923 21770 10624 6914 14769 6728 6305 9031 2003 2764 1999 2119 8948 1010 2007 2312 2752 1997 2598 1011 3221 2557 4181 17759 1998 1013 2030 102\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   label: 1 (id = 1)\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   guid: test-2\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   tokens: [CLS] tip of the left trans ##ju ##gul ##ar swan - gan ##z cat ##het ##er projects over the right vent ##ric ##le below the out ##flow tract . no media ##sti ##nal widening or app ##re ##cia ##ble pl ##eur ##al e ##ff ##usion . no p ##ne ##um ##otho ##ra ##x . et tube is at the thor ##ac ##ic inlet . sin ##us rhythm with border ##line sin ##us ta ##chy ##card ##ia . generalized low voltage . delayed r wave progression with late pre ##cor ##dial q ##rs transition . findings are non - specific . clinical correlation is suggested . since the previous tracing of sin ##us ta ##chy ##card ##ia rate is slower and right pre ##cor ##dial lead t wave changes appear more prominent . tracing # two renal failure , acute ( acute renal failure , ar ##f ) assessment : pt with low u ##o from or making only four hundred and thirty - five for entire length of case h / o of at ##n / ar ##f ict ##eric , clear urine , some sediment hu ##o zero - 80 ##cc / h action : intermittent bo ##lus ##es of fur ##ose ##mide in between and following pr ##bc ##s hu ##o monitored and documented transplant / sic ##u teams aware of hu ##o or lack there of pt made kv ##o d / t fl ##d intake in products from or and here in sic ##u labs q four hr response : no response from fur ##ose ##mide labs showing cr taking bump from one . zero to one . eight plan : continue to monitor at ##n continue to monitor cr / bun / renal function respiratory failure , acute ( not ar ##ds / ) assessment : pt initially on ac 850 ##x ##14 , forty % from or an ##esthesia settings ab ##gs getting more al ##kal ##otic post - fur ##ose ##mide doses o ##2 sat ##s ~ ninety - five % - ninety - seven % briefly dropping o ##2 sat ##s ~ ninety and not rebound ##ing with su ##ction action : ab ##gs drawn q ##2 rt in to evaluate pt with drop in sat , rebound ##ing spontaneously after several inter ##vet ##ions attempted ab ##gs also reflecting a drop in pa ##0 ##2 tv lowered to eight hundred as pt growing more al ##kal ##otic response : ab ##gs trend ##ing al ##kal ##otic plan : ab ##gs followed q ##2 hr f / u at six hundred for ab ##g on adjusted vent settings renal failure , acute ( acute renal failure , ar ##f ) assessment : pt with low u ##o from or making only four hundred and thirty - five for entire length of case h / o of at ##n / ar ##f ict ##eric , clear urine , some sediment hu ##o zero - 80 ##cc / h action : intermittent bo ##lus ##es of fur ##ose ##mide in between and following pr ##bc [SEP]\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   input_ids: 101 5955 1997 1996 2187 9099 9103 24848 2906 10677 1011 25957 2480 4937 27065 2121 3934 2058 1996 2157 18834 7277 2571 2917 1996 2041 12314 12859 1012 2053 2865 16643 12032 17973 2030 10439 2890 7405 3468 20228 11236 2389 1041 4246 14499 1012 2053 1052 2638 2819 29288 2527 2595 1012 3802 7270 2003 2012 1996 15321 6305 2594 15824 1012 8254 2271 6348 2007 3675 4179 8254 2271 11937 11714 11522 2401 1012 18960 2659 10004 1012 8394 1054 4400 14967 2007 2397 3653 27108 27184 1053 2869 6653 1012 9556 2024 2512 1011 3563 1012 6612 16902 2003 4081 1012 2144 1996 3025 16907 1997 8254 2271 11937 11714 11522 2401 3446 2003 12430 1998 2157 3653 27108 27184 2599 1056 4400 3431 3711 2062 4069 1012 16907 1001 2048 25125 4945 1010 11325 1006 11325 25125 4945 1010 12098 2546 1007 7667 1024 13866 2007 2659 1057 2080 2013 2030 2437 2069 2176 3634 1998 4228 1011 2274 2005 2972 3091 1997 2553 1044 1013 1051 1997 2012 2078 1013 12098 2546 25891 22420 1010 3154 17996 1010 2070 19671 15876 2080 5717 1011 3770 9468 1013 1044 2895 1024 23852 8945 7393 2229 1997 6519 9232 24284 1999 2090 1998 2206 10975 9818 2015 15876 2080 17785 1998 8832 22291 1013 14387 2226 2780 5204 1997 15876 2080 2030 3768 2045 1997 13866 2081 24888 2080 1040 1013 1056 13109 2094 13822 1999 3688 2013 2030 1998 2182 1999 14387 2226 13625 1053 2176 17850 3433 1024 2053 3433 2013 6519 9232 24284 13625 4760 13675 2635 16906 2013 2028 1012 5717 2000 2028 1012 2809 2933 1024 3613 2000 8080 2012 2078 3613 2000 8080 13675 1013 21122 1013 25125 3853 16464 4945 1010 11325 1006 2025 12098 5104 1013 1007 7667 1024 13866 3322 2006 9353 15678 2595 16932 1010 5659 1003 2013 2030 2019 25344 10906 11113 5620 2893 2062 2632 12902 20214 2695 1011 6519 9232 24284 21656 1051 2475 2938 2015 1066 13568 1011 2274 1003 1011 13568 1011 2698 1003 4780 7510 1051 2475 2938 2015 1066 13568 1998 2025 27755 2075 2007 10514 7542 2895 1024 11113 5620 4567 1053 2475 19387 1999 2000 16157 13866 2007 4530 1999 2938 1010 27755 2075 27491 2044 2195 6970 19510 8496 4692 11113 5620 2036 10842 1037 4530 1999 6643 2692 2475 2694 6668 2000 2809 3634 2004 13866 3652 2062 2632 12902 20214 3433 1024 11113 5620 9874 2075 2632 12902 20214 2933 1024 11113 5620 2628 1053 2475 17850 1042 1013 1057 2012 2416 3634 2005 11113 2290 2006 10426 18834 10906 25125 4945 1010 11325 1006 11325 25125 4945 1010 12098 2546 1007 7667 1024 13866 2007 2659 1057 2080 2013 2030 2437 2069 2176 3634 1998 4228 1011 2274 2005 2972 3091 1997 2553 1044 1013 1051 1997 2012 2078 1013 12098 2546 25891 22420 1010 3154 17996 1010 2070 19671 15876 2080 5717 1011 3770 9468 1013 1044 2895 1024 23852 8945 7393 2229 1997 6519 9232 24284 1999 2090 1998 2206 10975 9818 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:25:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   label: 1 (id = 1)\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   guid: test-3\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   tokens: [CLS] transplant / sic ##u teams aware of hu ##o or lack there of pt made kv ##o d / t fl ##d intake in products from or and here in sic ##u labs q four hr response : no response from fur ##ose ##mide labs showing cr taking bump from one . zero to one . eight plan : continue to monitor at ##n continue to monitor cr / bun / renal function respiratory failure , acute ( not ar ##ds / ) assessment : pt initially on ac 850 ##x ##14 , forty % from or an ##esthesia settings ab ##gs getting more al ##kal ##otic post - fur ##ose ##mide doses o ##2 sat ##s ~ ninety - five % - ninety - seven % briefly dropping o ##2 sat ##s ~ ninety and not rebound ##ing with su ##ction action : ab ##gs drawn q ##2 rt in to evaluate pt with drop in sat , rebound ##ing spontaneously after several inter ##vet ##ions attempted su ##ction ##ing , c ##x ##r , re - positioning ab ##gs also reflecting a drop in pa ##0 ##2 tv lowered to eight hundred as pt growing more al ##kal ##otic response : ab ##gs trend ##ing al ##kal ##otic plan : ab ##gs followed q ##2 hr f / u at six hundred for ab ##g on adjusted vent settings transplant , liver assessment : pt received @ one thousand , nine hundred shortly after arriving in sic ##u from or post - liver transplant ng ##t to lc ##ws for bright to dark red bloody output na ##res packed as pt was o ##oz ##ing blood from back of mouth / na ##res in or absent bow ##el sounds inc ##ision with initial ds ##g intact , re - info ##rce ##d several times throughout shift cop ##ious s / s d ##ng se ##date ##d on prop ##of ##ol not withdrawing to painful stimuli , but facial grim ##acing , able to move head / neck to stimuli , ex ##tre ##mit ##ies very ed ##ema ##tou ##s / heavy pupils slug ##gis ##h at 3 ##mm cc ##o initiated at change of shift , swan - gan ##z cat ##het ##er intact , fifty - five cm pt hyper ##dy ##nami ##c elevated pa numbers , hyper ##tens ##ive to sb ##p 160 ##s , ta ##chy ##card ##ic to 120 ##s jp drains x ##2 ( medial / lateral ) to self su ##ction putting out ~ five - 50 ##cc / hr action : c ##x ##r obtained iv ##f at 125 ##cc / hr changed to kv ##o as pt receiving l ##g am ##ts of fl ##d in products labs q 4 ##hr , ab ##gs q ##2 ##hr drains emptied q one hr ab ##x , cell ##ce ##pt as ordered pr ##bc ##s x ##3 units for goal hc ##t > thirty intermittent bo ##lus ##es fur ##ose ##mide in between / after response : hc ##t ~ twenty - [SEP]\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   input_ids: 101 22291 1013 14387 2226 2780 5204 1997 15876 2080 2030 3768 2045 1997 13866 2081 24888 2080 1040 1013 1056 13109 2094 13822 1999 3688 2013 2030 1998 2182 1999 14387 2226 13625 1053 2176 17850 3433 1024 2053 3433 2013 6519 9232 24284 13625 4760 13675 2635 16906 2013 2028 1012 5717 2000 2028 1012 2809 2933 1024 3613 2000 8080 2012 2078 3613 2000 8080 13675 1013 21122 1013 25125 3853 16464 4945 1010 11325 1006 2025 12098 5104 1013 1007 7667 1024 13866 3322 2006 9353 15678 2595 16932 1010 5659 1003 2013 2030 2019 25344 10906 11113 5620 2893 2062 2632 12902 20214 2695 1011 6519 9232 24284 21656 1051 2475 2938 2015 1066 13568 1011 2274 1003 1011 13568 1011 2698 1003 4780 7510 1051 2475 2938 2015 1066 13568 1998 2025 27755 2075 2007 10514 7542 2895 1024 11113 5620 4567 1053 2475 19387 1999 2000 16157 13866 2007 4530 1999 2938 1010 27755 2075 27491 2044 2195 6970 19510 8496 4692 10514 7542 2075 1010 1039 2595 2099 1010 2128 1011 19120 11113 5620 2036 10842 1037 4530 1999 6643 2692 2475 2694 6668 2000 2809 3634 2004 13866 3652 2062 2632 12902 20214 3433 1024 11113 5620 9874 2075 2632 12902 20214 2933 1024 11113 5620 2628 1053 2475 17850 1042 1013 1057 2012 2416 3634 2005 11113 2290 2006 10426 18834 10906 22291 1010 11290 7667 1024 13866 2363 1030 2028 4595 1010 3157 3634 3859 2044 7194 1999 14387 2226 2013 2030 2695 1011 11290 22291 12835 2102 2000 29215 9333 2005 4408 2000 2601 2417 6703 6434 6583 6072 8966 2004 13866 2001 1051 18153 2075 2668 2013 2067 1997 2677 1013 6583 6072 1999 2030 9962 6812 2884 4165 4297 19969 2007 3988 16233 2290 10109 1010 2128 1011 18558 19170 2094 2195 2335 2802 5670 8872 6313 1055 1013 1055 1040 3070 7367 13701 2094 2006 17678 11253 4747 2025 21779 2000 9145 22239 1010 2021 13268 11844 26217 1010 2583 2000 2693 2132 1013 3300 2000 22239 1010 4654 7913 22930 3111 2200 3968 14545 24826 2015 1013 3082 7391 23667 17701 2232 2012 1017 7382 10507 2080 7531 2012 2689 1997 5670 1010 10677 1011 25957 2480 4937 27065 2121 10109 1010 5595 1011 2274 4642 13866 23760 5149 28987 2278 8319 6643 3616 1010 23760 25808 3512 2000 24829 2361 8148 2015 1010 11937 11714 11522 2594 2000 6036 2015 16545 18916 1060 2475 1006 23828 1013 11457 1007 2000 2969 10514 7542 5128 2041 1066 2274 1011 2753 9468 1013 17850 2895 1024 1039 2595 2099 4663 4921 2546 2012 8732 9468 1013 17850 2904 2000 24888 2080 2004 13866 4909 1048 2290 2572 3215 1997 13109 2094 1999 3688 13625 1053 1018 8093 1010 11113 5620 1053 2475 8093 18916 21764 1053 2028 17850 11113 2595 1010 3526 3401 13876 2004 3641 10975 9818 2015 1060 2509 3197 2005 3125 16731 2102 1028 4228 23852 8945 7393 2229 6519 9232 24284 1999 2090 1013 2044 3433 1024 16731 2102 1066 3174 1011 102\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:25:06 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   label: 1 (id = 1)\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   *** Example ***\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   guid: test-4\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   tokens: [CLS] hc ##t twenty - six . two % with four hundred and thirty labs plan : monitor hc ##t every two hours with ab ##gs and labs every four hours . ( goal hc ##t > thirty ) trans ##fus ##e pt according to goals ? additional pr ##bc ##s to treat hc ##t ~ twenty - six - twenty - seven for goal > thirty continue pathway sic ##u hp ##i : 45 ##yo m w / et ##oh ci ##rr ##hosis , mel ##d twenty - eight , child class c ci ##rr ##hosis admitted for liver transplant . denies change in health since previous admission eight / . admitted to for fever ##s , an ##emia , as ##cite ##s , ar ##f . hospital course , treated w / z ##os ##yn for c . per ##fr ##ingen ##s ba ##cter ##emia . para ##cent ##esis negative for sb ##p . e ##g ##d w / grade i var ##ices . ar ##f responded to oct ##re ##otide , mid ##od ##rine . chief complaint : liver failure pm ##h ##x : pm ##h : ( ) : et ##oh ci ##rr ##hosis , go ##ut , h ##t ##n now norm ##ote ##ns ##ive off med ##s , gr one es ##op ##ha ##ge ##al var ##ices , c per ##fr ##ingen ##s ba ##cter ##emia . . ps ##h : remote app ##end ##ect ##omy . : f ##olic acid one , th ##iam ##ine hc ##l one hundred , ur ##so ##dio ##l three hundred ' ' ' , rani ##ti ##dine hc ##l one hundred and fifty ' ' , lac ##tu ##los ##e , fur ##ose ##mide twenty , sp ##iro ##no ##la ##cton ##e one hundred , z ##of ##ran four , ma ##alo ##x , ri ##fa ##xi ##min two hundred ' ' ' current medications : fifteen . mor ##phine sulfate one - four mg iv q ##1 ##h : pr ##n pain order date : @ one thousand , seven hundred and fifteen sixteen . my ##co ##ph ##eno ##late mo ##fe ##ti ##l suspension one thousand mg po / ng order date : @ one thousand , seven hundred and fifteen three . amp ##ici ##llin - sul ##ba ##cta ##m three gm iv q ##6 ##h duration : three doses order date : @ one thousand , seven hundred and fifteen seventeen . on ##dan ##set ##ron four mg iv q ##8 ##h : pr ##n nausea / vomiting order date : @ one thousand , seven hundred and fifteen four . ch ##lor ##he ##xi ##dine g ##lu ##cona ##te zero . twelve % oral ri ##nse fifteen ml oral use only if patient is on mechanical ventilation . order date : @ one thousand , seven hundred and fifty - eight eighteen . pan ##top ##raz ##ole forty mg iv q ##24 ##h order date : @ one thousand , seven hundred and fifteen five . doc ##usa ##te sodium one hundred mg [SEP]\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   input_ids: 101 16731 2102 3174 1011 2416 1012 2048 1003 2007 2176 3634 1998 4228 13625 2933 1024 8080 16731 2102 2296 2048 2847 2007 11113 5620 1998 13625 2296 2176 2847 1012 1006 3125 16731 2102 1028 4228 1007 9099 25608 2063 13866 2429 2000 3289 1029 3176 10975 9818 2015 2000 7438 16731 2102 1066 3174 1011 2416 1011 3174 1011 2698 2005 3125 1028 4228 3613 12732 14387 2226 6522 2072 1024 3429 7677 1049 1059 1013 3802 11631 25022 12171 25229 1010 11463 2094 3174 1011 2809 1010 2775 2465 1039 25022 12171 25229 4914 2005 11290 22291 1012 23439 2689 1999 2740 2144 3025 9634 2809 1013 1012 4914 2000 2005 9016 2015 1010 2019 17577 1010 2004 17847 2015 1010 12098 2546 1012 2902 2607 1010 5845 1059 1013 1062 2891 6038 2005 1039 1012 2566 19699 15542 2015 8670 21162 17577 1012 11498 13013 19009 4997 2005 24829 2361 1012 1041 2290 2094 1059 1013 3694 1045 13075 23522 1012 12098 2546 5838 2000 13323 2890 26601 1010 3054 7716 11467 1012 2708 12087 1024 11290 4945 7610 2232 2595 1024 7610 2232 1024 1006 1007 1024 3802 11631 25022 12171 25229 1010 2175 4904 1010 1044 2102 2078 2085 13373 12184 3619 3512 2125 19960 2015 1010 24665 2028 9686 7361 3270 3351 2389 13075 23522 1010 1039 2566 19699 15542 2015 8670 21162 17577 1012 1012 8827 2232 1024 6556 10439 10497 22471 16940 1012 1024 1042 23518 5648 2028 1010 16215 25107 3170 16731 2140 2028 3634 1010 24471 6499 20617 2140 2093 3634 1005 1005 1005 1010 21617 3775 10672 16731 2140 2028 3634 1998 5595 1005 1005 1010 18749 8525 10483 2063 1010 6519 9232 24284 3174 1010 11867 9711 3630 2721 28312 2063 2028 3634 1010 1062 11253 5521 2176 1010 5003 23067 2595 1010 15544 7011 9048 10020 2048 3634 1005 1005 1005 2783 20992 1024 5417 1012 22822 20738 26754 2028 1011 2176 11460 4921 1053 2487 2232 1024 10975 2078 3255 2344 3058 1024 1030 2028 4595 1010 2698 3634 1998 5417 7032 1012 2026 3597 8458 16515 13806 9587 7959 3775 2140 8636 2028 4595 11460 13433 1013 12835 2344 3058 1024 1030 2028 4595 1010 2698 3634 1998 5417 2093 1012 23713 28775 21202 1011 21396 3676 25572 2213 2093 13938 4921 1053 2575 2232 9367 1024 2093 21656 2344 3058 1024 1030 2028 4595 1010 2698 3634 1998 5417 9171 1012 2006 7847 13462 4948 2176 11460 4921 1053 2620 2232 1024 10975 2078 19029 1013 24780 2344 3058 1024 1030 2028 4595 1010 2698 3634 1998 5417 2176 1012 10381 10626 5369 9048 10672 1043 7630 24366 2618 5717 1012 4376 1003 8700 15544 12325 5417 19875 8700 2224 2069 2065 5776 2003 2006 6228 19536 1012 2344 3058 1024 1030 2028 4595 1010 2698 3634 1998 5595 1011 2809 7763 1012 6090 14399 20409 9890 5659 11460 4921 1053 18827 2232 2344 3058 1024 1030 2028 4595 1010 2698 3634 1998 5417 2274 1012 9986 10383 2618 13365 2028 3634 11460 102\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "01/05/2022 12:25:06 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:25:06 - INFO - __main__ -   label: 1 (id = 1)\n"
     ]
    }
   ],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "    label_map[label] = i\n",
    "\n",
    "features = []\n",
    "for (ex_index, example) in enumerate(eval_examples):\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b) # text_b is not useful in our task\n",
    "\n",
    "    if tokens_b:\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(max_seq_length - 2)] # only trained with 0: 512-2\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0   0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens) \n",
    "    # BertTokenizer.from_pretrained('bert-base-uncased').convert_tokens_to_ids\n",
    "    # convert tokens to number (max 512)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    #print (example.label)\n",
    "    label_id = label_map[example.label]\n",
    "    if ex_index < 5:\n",
    "        logger.info(\"*** Example ***\")\n",
    "        logger.info(\"guid: %s\" % (example.guid))\n",
    "        logger.info(\"tokens: %s\" % \" \".join(\n",
    "                [str(x) for x in tokens]))\n",
    "        logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        logger.info(\n",
    "                \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "    features.append(\n",
    "            InputFeatures(input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 8254, 2271, 11937, 11714, 11522, 2401, 1012, 18960, 2659, 10004, 1012, 8394, 1054, 4400, 14967, 2007, 2397, 3653, 27108, 27184, 1053, 2869, 6653, 1012, 9556, 2024, 2512, 1011, 3563, 1012, 6612, 16902, 2003, 4081, 1012, 2144, 1996, 3025, 16907, 1997, 2168, 3058, 8254, 2271, 11937, 11714, 11522, 2401, 3446, 2003, 5514, 1012, 16907, 1001, 2028, 8254, 2271, 6348, 1012, 8394, 1054, 4400, 14967, 2007, 2397, 3653, 27108, 27184, 1053, 2869, 6653, 1012, 18960, 2659, 1053, 2869, 10004, 1012, 9556, 2024, 2512, 1011, 3563, 1012, 6612, 16902, 2003, 4081, 1012, 2144, 1996, 3025, 16907, 1997, 1996, 3446, 2003, 5514, 1998, 10004, 2003, 2896, 1012, 2516, 1024, 5659, 1011, 2274, 1061, 1013, 1051, 2158, 2007, 1044, 2595, 1997, 3802, 11631, 25022, 12171, 25229, 2040, 3369, 2005, 3740, 11290, 22291, 1012, 2253, 2000, 2030, 2572, 1998, 2363, 13866, 2013, 2030, 22480, 2028, 4595, 1010, 2809, 3634, 20014, 19761, 3064, 1998, 7367, 13701, 2094, 2006, 17678, 11253, 4747, 1012, 22291, 1010, 11290, 7667, 1024, 20014, 19761, 3064, 2006, 9353, 2012, 5659, 1003, 10882, 2080, 2475, 1012, 8948, 3154, 2000, 15911, 1999, 7888, 1012, 12835, 2102, 2000, 29215, 9333, 1012, 9962, 6812, 2884, 4165, 1012, 4895, 6072, 26029, 12742, 1012, 2025, 21779, 2000, 9145, 22239, 1012, 7391, 23667, 17701, 2232, 2012, 1017, 7382, 1012, 7367, 13701, 2094, 2006, 17678, 11253, 4747, 1012, 10507, 2080, 7531, 1012, 10677, 1011, 25957, 2480, 4937, 27065, 2121, 10109, 1012, 16545, 18916, 1060, 2475, 1006, 23828, 1013, 11457, 1007, 2000, 2969, 10514, 7542, 1012, 21419, 4297, 19969, 2007, 2235, 3815, 14262, 2080, 1011, 6369, 11987, 1012, 2895, 1024, 2035, 2695, 1011, 6728, 13625, 2741, 1012, 6819, 20142, 8780, 10250, 12322, 9250, 1012, 23969, 2290, 4663, 1012, 1039, 2595, 2099, 4663, 1012, 4921, 2546, 2012, 8732, 9468, 1013, 17850, 1012, 3433, 1024, 2053, 18834, 3431, 1012, 10514, 7542, 2075, 2005, 13594, 2102, 3595, 8496, 1010, 2235, 3815, 6703, 8700, 3595, 8496, 1012, 12835, 2102, 2007, 13594, 2102, 8310, 1997, 4408, 2417, 2668, 6434, 1012, 2053, 3431, 1999, 11265, 10976, 11360, 1012, 16545, 18916, 2007, 13594, 2102, 3815, 1997, 14262, 2080, 1011, 6369, 11987, 1012, 2933, 1024, 3613, 2000, 8080, 11265, 10976, 11360, 2296, 2176, 2847, 1012, 8080, 16731, 2102, 2296, 2048, 2847, 2007, 11113, 5620, 1998, 13625, 2296, 2176, 2847, 1012, 1006, 3125, 16731, 2102, 1028, 4228, 1007, 9099, 25608, 2063, 13866, 2429, 2000, 3289, 1012, 3613, 2000, 7438, 13866, 2566, 2695, 1011, 6728, 4449, 1998, 22291, 3673, 1012, 2516, 1024, 2363, 13866, 2013, 2030, 22480, 2028, 4595, 1010, 2809, 3634, 20014, 19761, 3064, 1998, 7367, 13701, 2094, 1012, 4895, 6072, 26029, 12742, 2006, 11360, 2025, 21779, 2030, 2334, 6026, 1012, 2274, 1024, 11977, 2572, 3108, 1006, 3653, 1011, 6728, 6643, 1004, 2474, 2102, 1007, 12528, 1001, 3114, 1024, 3653, 6728, 2143, 2966, 4650, 1024, 5659, 1011, 2274, 2095, 2214, 2158, 2007, 11290, 25022, 12171, 25229, 3114, 2005, 2023, 7749, 1024, 3653, 6728, 2143, 4954, 3191, 1024, 12997, 2546, 10722, 2063, 2274, 1024, 5595, 1011, 2093, 2572, 2053, 18583, 1012, 2235, 2187, 20228, 11236, 2389, 1041, 4246, 14499, 1012, 6540, 21449, 25707, 1012, 2345, 3189, 2381, 1024, 1037, 5659, 1011, 2274, 102] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(features):\n",
    "    if i < 1:\n",
    "        print(v.input_ids, v.input_mask, v.segment_ids, v.label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f6a8126f828>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512]) torch.Size([2, 512]) torch.Size([2, 512]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "batch_input_ids, batch_input_mask, batch_segment_ids, batch_label_ids = next(iter(eval_dataloader))\n",
    "print(batch_input_ids.size(), batch_input_mask.size(), batch_segment_ids.size(), batch_label_ids.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_config.json  pytorch_model.bin\r\n"
     ]
    }
   ],
   "source": [
    "! ls /data/users/haotiang/model/early_readmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 12:51:58 - INFO - modeling_readmission -   loading archive file /data/users/haotiang/model/early_readmission\n",
      "01/05/2022 12:51:58 - INFO - modeling_readmission -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "100%|| 2844/2844 [29:28<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_model = '/data/users/haotiang/model/early_readmission'\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model, 1) #load pretrained clinicalBERT model\n",
    "model.to(device)\n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "true_labels=[]\n",
    "pred_labels=[]\n",
    "logits_history=[]\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss, temp_logits = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        logits = model(input_ids,segment_ids,input_mask)\n",
    "\n",
    "    logits = torch.squeeze(m(logits)).detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "\n",
    "    outputs = np.asarray([1 if i else 0 for i in (logits.flatten()>=0.5)])\n",
    "    tmp_eval_accuracy=np.sum(outputs == label_ids)\n",
    "\n",
    "    true_labels = true_labels + label_ids.flatten().tolist()\n",
    "    pred_labels = pred_labels + outputs.flatten().tolist()\n",
    "    logits_history = logits_history + logits.flatten().tolist()\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "df = pd.DataFrame({'logits':logits_history, 'pred_label': pred_labels, 'label':true_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2022 15:28:21 - INFO - modeling_readmission -   loading archive file /data/users/haotiang/model/early_readmission\n",
      "01/05/2022 15:28:21 - INFO - modeling_readmission -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "  0%|          | 0/2844 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([4, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-9f9842784ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlabel_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtmp_eval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/modeling_readmission.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2517\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m         raise ValueError(\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\n\u001b[0;32m-> 2519\u001b[0;31m                          \"Please ensure they have the same size.\".format(target.size(), input.size()))\n\u001b[0m\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([4, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "bert_model = '/data/users/haotiang/model/early_readmission'\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model, 2) #load pretrained clinicalBERT model\n",
    "model.to(device)\n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "true_labels=[]\n",
    "pred_labels=[]\n",
    "logits_history=[]\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss, temp_logits = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        logits = model(input_ids,segment_ids,input_mask)\n",
    "\n",
    "    logits = torch.squeeze(m(logits)).detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "\n",
    "    outputs = np.asarray([1 if i else 0 for i in (logits.flatten()>=0.5)])\n",
    "    tmp_eval_accuracy=np.sum(outputs == label_ids)\n",
    "\n",
    "    true_labels = true_labels + label_ids.flatten().tolist()\n",
    "    pred_labels = pred_labels + outputs.flatten().tolist()\n",
    "    logits_history = logits_history + logits.flatten().tolist()\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "df = pd.DataFrame({'logits':logits_history, 'pred_label': pred_labels, 'label':true_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertForSequenceClassification.from_pretrained??\n",
    "# last layer needs to be trained, freeze the bert, to train the linear model. Here we are using random linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logits</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.508293</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.700661</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.661133</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.649345</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.986051</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>0.297631</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5683</th>\n",
       "      <td>0.543576</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5684</th>\n",
       "      <td>0.511333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5685</th>\n",
       "      <td>0.489785</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5686</th>\n",
       "      <td>0.329475</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5687 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        logits  pred_label  label\n",
       "0     0.508293           1      1\n",
       "1     0.700661           1      1\n",
       "2     0.661133           1      1\n",
       "3     0.649345           1      1\n",
       "4     0.986051           1      1\n",
       "...        ...         ...    ...\n",
       "5682  0.297631           0      0\n",
       "5683  0.543576           1      0\n",
       "5684  0.511333           1      0\n",
       "5685  0.489785           0      0\n",
       "5686  0.329475           0      0\n",
       "\n",
       "[5687 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'do_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-da532e0f152e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'do_train' is not defined"
     ]
    }
   ],
   "source": [
    "do_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([  101,  8254,  2271, 11937, 11714, 11522,  2401,  1012, 18960,  2659,\n",
      "        10004,  1012,  8394,  1054,  4400, 14967,  2007,  2397,  3653, 27108,\n",
      "        27184,  1053,  2869,  6653,  1012,  9556,  2024,  2512,  1011,  3563,\n",
      "         1012,  6612, 16902,  2003,  4081,  1012,  2144,  1996,  3025, 16907,\n",
      "         1997,  2168,  3058,  8254,  2271, 11937, 11714, 11522,  2401,  3446,\n",
      "         2003,  5514,  1012, 16907,  1001,  2028,  8254,  2271,  6348,  1012,\n",
      "         8394,  1054,  4400, 14967,  2007,  2397,  3653, 27108, 27184,  1053,\n",
      "         2869,  6653,  1012, 18960,  2659,  1053,  2869, 10004,  1012,  9556,\n",
      "         2024,  2512,  1011,  3563,  1012,  6612, 16902,  2003,  4081,  1012,\n",
      "         2144,  1996,  3025, 16907,  1997,  1996,  3446,  2003,  5514,  1998,\n",
      "        10004,  2003,  2896,  1012,  2516,  1024,  5659,  1011,  2274,  1061,\n",
      "         1013,  1051,  2158,  2007,  1044,  2595,  1997,  3802, 11631, 25022,\n",
      "        12171, 25229,  2040,  3369,  2005,  3740, 11290, 22291,  1012,  2253,\n",
      "         2000,  2030,  2572,  1998,  2363, 13866,  2013,  2030, 22480,  2028,\n",
      "         4595,  1010,  2809,  3634, 20014, 19761,  3064,  1998,  7367, 13701,\n",
      "         2094,  2006, 17678, 11253,  4747,  1012, 22291,  1010, 11290,  7667,\n",
      "         1024, 20014, 19761,  3064,  2006,  9353,  2012,  5659,  1003, 10882,\n",
      "         2080,  2475,  1012,  8948,  3154,  2000, 15911,  1999,  7888,  1012,\n",
      "        12835,  2102,  2000, 29215,  9333,  1012,  9962,  6812,  2884,  4165,\n",
      "         1012,  4895,  6072, 26029, 12742,  1012,  2025, 21779,  2000,  9145,\n",
      "        22239,  1012,  7391, 23667, 17701,  2232,  2012,  1017,  7382,  1012,\n",
      "         7367, 13701,  2094,  2006, 17678, 11253,  4747,  1012, 10507,  2080,\n",
      "         7531,  1012, 10677,  1011, 25957,  2480,  4937, 27065,  2121, 10109,\n",
      "         1012, 16545, 18916,  1060,  2475,  1006, 23828,  1013, 11457,  1007,\n",
      "         2000,  2969, 10514,  7542,  1012, 21419,  4297, 19969,  2007,  2235,\n",
      "         3815, 14262,  2080,  1011,  6369, 11987,  1012,  2895,  1024,  2035,\n",
      "         2695,  1011,  6728, 13625,  2741,  1012,  6819, 20142,  8780, 10250,\n",
      "        12322,  9250,  1012, 23969,  2290,  4663,  1012,  1039,  2595,  2099,\n",
      "         4663,  1012,  4921,  2546,  2012,  8732,  9468,  1013, 17850,  1012,\n",
      "         3433,  1024,  2053, 18834,  3431,  1012, 10514,  7542,  2075,  2005,\n",
      "        13594,  2102,  3595,  8496,  1010,  2235,  3815,  6703,  8700,  3595,\n",
      "         8496,  1012, 12835,  2102,  2007, 13594,  2102,  8310,  1997,  4408,\n",
      "         2417,  2668,  6434,  1012,  2053,  3431,  1999, 11265, 10976, 11360,\n",
      "         1012, 16545, 18916,  2007, 13594,  2102,  3815,  1997, 14262,  2080,\n",
      "         1011,  6369, 11987,  1012,  2933,  1024,  3613,  2000,  8080, 11265,\n",
      "        10976, 11360,  2296,  2176,  2847,  1012,  8080, 16731,  2102,  2296,\n",
      "         2048,  2847,  2007, 11113,  5620,  1998, 13625,  2296,  2176,  2847,\n",
      "         1012,  1006,  3125, 16731,  2102,  1028,  4228,  1007,  9099, 25608,\n",
      "         2063, 13866,  2429,  2000,  3289,  1012,  3613,  2000,  7438, 13866,\n",
      "         2566,  2695,  1011,  6728,  4449,  1998, 22291,  3673,  1012,  2516,\n",
      "         1024,  2363, 13866,  2013,  2030, 22480,  2028,  4595,  1010,  2809,\n",
      "         3634, 20014, 19761,  3064,  1998,  7367, 13701,  2094,  1012,  4895,\n",
      "         6072, 26029, 12742,  2006, 11360,  2025, 21779,  2030,  2334,  6026,\n",
      "         1012,  2274,  1024, 11977,  2572,  3108,  1006,  3653,  1011,  6728,\n",
      "         6643,  1004,  2474,  2102,  1007, 12528,  1001,  3114,  1024,  3653,\n",
      "         6728,  2143,  2966,  4650,  1024,  5659,  1011,  2274,  2095,  2214,\n",
      "         2158,  2007, 11290, 25022, 12171, 25229,  3114,  2005,  2023,  7749,\n",
      "         1024,  3653,  6728,  2143,  4954,  3191,  1024, 12997,  2546, 10722,\n",
      "         2063,  2274,  1024,  5595,  1011,  2093,  2572,  2053, 18583,  1012,\n",
      "         2235,  2187, 20228, 11236,  2389,  1041,  4246, 14499,  1012,  6540,\n",
      "        21449, 25707,  1012,  2345,  3189,  2381,  1024,  1037,  5659,  1011,\n",
      "         2274,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor(1))\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(eval_data):\n",
    "    if i == 0: # only check with the first row\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512]) torch.Size([512]) torch.Size([512]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(eval_data):\n",
    "    if i < 1: # only check with the first row\n",
    "        input_ids, input_mask, segment_ids, label_ids = v\n",
    "        print(input_ids.size(), input_mask.size(), segment_ids.size(), label_ids.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "true_labels=[]\n",
    "pred_labels=[]\n",
    "logits_history=[]\n",
    "\n",
    "for i, v in enumerate(eval_dataloader):\n",
    "    if i < 1: # only check with the first row, repete twice for each row\n",
    "        input_ids, input_mask, segment_ids, label_ids = v\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss, temp_logits = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            logits = model(input_ids,segment_ids,input_mask)\n",
    "        logits = torch.squeeze(m(logits)).detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        outputs = np.asarray([1 if i else 0 for i in (logits.flatten()>=0.5)]) # key\n",
    "        tmp_eval_accuracy=np.sum(outputs == label_ids) # 2 numbers are added (0,0) (0,1) (1,0) (1, 1)\n",
    "        \n",
    "        true_labels = true_labels + label_ids.flatten().tolist()\n",
    "        pred_labels = pred_labels + outputs.flatten().tolist()\n",
    "        logits_history = logits_history + logits.flatten().tolist()\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy # sum of accuracy from batch size 2\n",
    "\n",
    "        nb_eval_examples += input_ids.size(0) # add 2 each loop\n",
    "        nb_eval_steps += 1 # add 1 each loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  8254,  2271,  ...,  1011,  2274,   102],\n",
       "        [  101,  3653, 25918,  ...,  1013,  2030,   102]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5162)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0332],\n",
       "        [0.8504]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5082934 , 0.70066136], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.squeeze(m(logits)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5082934 , 0.70066136], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([1 if i else 0 for i in (logits.flatten()>=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1], [1, 1], [0.5082933902740479, 0.7006613612174988])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels, pred_labels, logits_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5162)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5162135362625122"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_eval_loss.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/haotiang/notebook\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/Algorithm_of_Scalable_Readmission.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = logits_history\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
    "df_test['pred_score'] = score\n",
    "df_sort = df_test.sort_values(by=['ID'])\n",
    "\n",
    "#score \n",
    "# algorithm is based on above \n",
    "# c is set to 2 as eval_batch_size = 2 # default\n",
    "temp = (df_sort.groupby(['ID'])['pred_score'].agg(max)+df_sort.groupby(['ID'])['pred_score'].agg(sum)/2)/(1+df_sort.groupby(['ID'])['pred_score'].agg(len)/2)\n",
    "x = df_sort.groupby(['ID'])['Label'].agg(np.min).values # although min is used here, id is actually the same\n",
    "df_out = pd.DataFrame({'logits': temp.values, 'ID': x})\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(x, temp.values)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Val (area = {:.3f})'.format(auc_score))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string = 'auroc_clinicalbert_'+readmission_mode+'.png'\n",
    "#plt.savefig(os.path.join(output_dir, string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_score(df, score):\n",
    "    df['pred_score'] = score\n",
    "    df_sort = df.sort_values(by=['ID'])\n",
    "    #score \n",
    "    temp = (df_sort.groupby(['ID'])['pred_score'].agg(max)+df_sort.groupby(['ID'])['pred_score'].agg(sum)/2)/(1+df_sort.groupby(['ID'])['pred_score'].agg(len)/2)\n",
    "    x = df_sort.groupby(['ID'])['Label'].agg(np.min).values\n",
    "    df_out = pd.DataFrame({'logits': temp.values, 'ID': x})\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(x, temp.values)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label='Val (area = {:.3f})'.format(auc_score))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    string = 'auroc_clinicalbert_'+readmission_mode+'.png'\n",
    "    plt.savefig(os.path.join(output_dir, string))\n",
    "\n",
    "    return fpr, tpr, df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/haotiang/notebook'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = logits_history\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
    "df_test['pred_score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Label</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>sinus tachycardia. generalized low voltage. de...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.508293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>537</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>preoperative films for liver transplant. techn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>538</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>tip of the left transjugular swan-ganz cathete...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>539</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>transplant/sicu teams aware of huo or lack the...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.649345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>hct twenty-six.two% with four hundred and thir...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.986051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>401376</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>for obstruction. nursing progress note full co...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.297631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5683</th>\n",
       "      <td>401377</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>also displayed. ct of the chest with and witho...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.543576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5684</th>\n",
       "      <td>401378</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>abdomen. ct of the pelvis with and without iv ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5685</th>\n",
       "      <td>401379</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>totally drained 3800cc. later around two thous...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.489785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5686</th>\n",
       "      <td>401380</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>stable post-op. no further bleeding w/ stable ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.329475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5687 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        ID                                               TEXT  \\\n",
       "0            536  100133.0  sinus tachycardia. generalized low voltage. de...   \n",
       "1            537  100133.0  preoperative films for liver transplant. techn...   \n",
       "2            538  100133.0  tip of the left transjugular swan-ganz cathete...   \n",
       "3            539  100133.0  transplant/sicu teams aware of huo or lack the...   \n",
       "4            540  100133.0  hct twenty-six.two% with four hundred and thir...   \n",
       "...          ...       ...                                                ...   \n",
       "5682      401376  199930.0  for obstruction. nursing progress note full co...   \n",
       "5683      401377  199930.0  also displayed. ct of the chest with and witho...   \n",
       "5684      401378  199930.0  abdomen. ct of the pelvis with and without iv ...   \n",
       "5685      401379  199930.0  totally drained 3800cc. later around two thous...   \n",
       "5686      401380  199930.0  stable post-op. no further bleeding w/ stable ...   \n",
       "\n",
       "      Label  pred_score  \n",
       "0       1.0    0.508293  \n",
       "1       1.0    0.700661  \n",
       "2       1.0    0.661133  \n",
       "3       1.0    0.649345  \n",
       "4       1.0    0.986051  \n",
       "...     ...         ...  \n",
       "5682    0.0    0.297631  \n",
       "5683    0.0    0.543576  \n",
       "5684    0.0    0.511333  \n",
       "5685    0.0    0.489785  \n",
       "5686    0.0    0.329475  \n",
       "\n",
       "[5687 rows x 5 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Label</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>sinus tachycardia. generalized low voltage. de...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.508293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>558</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>acute distress cardiovascular: (rhythm: regula...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.506082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>559</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>care nutrition: glycemic control: insulin infu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.579559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>560</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>right portal vein which may be technical in na...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.615552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>561</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>a central line change. study: supine portable ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.923494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5685</th>\n",
       "      <td>401379</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>totally drained 3800cc. later around two thous...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.489785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5680</th>\n",
       "      <td>401374</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>impression: ap chest reviewed in the absence o...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.308584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5679</th>\n",
       "      <td>401373</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>baseline artifact. sinus rhythm with borderlin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.523618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5681</th>\n",
       "      <td>401375</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>bowel, and left colon. sma arteriograms demons...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.321223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5686</th>\n",
       "      <td>401380</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>stable post-op. no further bleeding w/ stable ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.329475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5687 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        ID                                               TEXT  \\\n",
       "0            536  100133.0  sinus tachycardia. generalized low voltage. de...   \n",
       "22           558  100133.0  acute distress cardiovascular: (rhythm: regula...   \n",
       "23           559  100133.0  care nutrition: glycemic control: insulin infu...   \n",
       "24           560  100133.0  right portal vein which may be technical in na...   \n",
       "25           561  100133.0  a central line change. study: supine portable ...   \n",
       "...          ...       ...                                                ...   \n",
       "5685      401379  199930.0  totally drained 3800cc. later around two thous...   \n",
       "5680      401374  199930.0  impression: ap chest reviewed in the absence o...   \n",
       "5679      401373  199930.0  baseline artifact. sinus rhythm with borderlin...   \n",
       "5681      401375  199930.0  bowel, and left colon. sma arteriograms demons...   \n",
       "5686      401380  199930.0  stable post-op. no further bleeding w/ stable ...   \n",
       "\n",
       "      Label  pred_score  \n",
       "0       1.0    0.508293  \n",
       "22      1.0    0.506082  \n",
       "23      1.0    0.579559  \n",
       "24      1.0    0.615552  \n",
       "25      1.0    0.923494  \n",
       "...     ...         ...  \n",
       "5685    0.0    0.489785  \n",
       "5680    0.0    0.308584  \n",
       "5679    0.0    0.523618  \n",
       "5681    0.0    0.321223  \n",
       "5686    0.0    0.329475  \n",
       "\n",
       "[5687 rows x 5 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = (df_sort.groupby(['ID'])['pred_score'].agg(max)+df_sort.groupby(['ID'])['pred_score'].agg(sum)/2)/(1+df_sort.groupby(['ID'])['pred_score'].agg(len)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "100133.0    0.715474\n",
       "100143.0    0.270472\n",
       "100294.0    0.645512\n",
       "100380.0    0.561849\n",
       "100485.0    0.319974\n",
       "              ...   \n",
       "199362.0    0.443447\n",
       "199452.0    0.433622\n",
       "199616.0    0.576081\n",
       "199628.0    0.575013\n",
       "199930.0    0.441238\n",
       "Name: pred_score, Length: 518, dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "100133.0    1.0\n",
       "100143.0    0.0\n",
       "100294.0    0.0\n",
       "100380.0    1.0\n",
       "100485.0    1.0\n",
       "           ... \n",
       "199362.0    1.0\n",
       "199452.0    0.0\n",
       "199616.0    1.0\n",
       "199628.0    1.0\n",
       "199930.0    0.0\n",
       "Name: Label, Length: 518, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sort.groupby(['ID'])['Label'].agg(np.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_sort.groupby(['ID'])['Label'].agg(np.min).values # although min is used here, id is actually the same\n",
    "df_out = pd.DataFrame({'logits': temp.values, 'ID': x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logits</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.715474</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.270472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.645512</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.561849</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.319974</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0.443447</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0.433622</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0.576081</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>0.575013</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>0.441238</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       logits   ID\n",
       "0    0.715474  1.0\n",
       "1    0.270472  0.0\n",
       "2    0.645512  0.0\n",
       "3    0.561849  1.0\n",
       "4    0.319974  1.0\n",
       "..        ...  ...\n",
       "513  0.443447  1.0\n",
       "514  0.433622  0.0\n",
       "515  0.576081  1.0\n",
       "516  0.575013  1.0\n",
       "517  0.441238  0.0\n",
       "\n",
       "[518 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.        , 0.00392157, 0.00392157, 0.00784314,\n",
       "        0.00784314, 0.01176471, 0.01176471, 0.01568627, 0.01568627,\n",
       "        0.01960784, 0.01960784, 0.02352941, 0.02352941, 0.02745098,\n",
       "        0.02745098, 0.03137255, 0.03137255, 0.03921569, 0.03921569,\n",
       "        0.04313725, 0.04313725, 0.04705882, 0.04705882, 0.05098039,\n",
       "        0.05098039, 0.05882353, 0.05882353, 0.06666667, 0.06666667,\n",
       "        0.0745098 , 0.0745098 , 0.07843137, 0.07843137, 0.08627451,\n",
       "        0.08627451, 0.09411765, 0.09411765, 0.09803922, 0.09803922,\n",
       "        0.10196078, 0.10196078, 0.10588235, 0.10588235, 0.10980392,\n",
       "        0.10980392, 0.11372549, 0.11372549, 0.11764706, 0.11764706,\n",
       "        0.12156863, 0.12156863, 0.1254902 , 0.1254902 , 0.12941176,\n",
       "        0.12941176, 0.13333333, 0.13333333, 0.1372549 , 0.1372549 ,\n",
       "        0.14509804, 0.14509804, 0.14901961, 0.14901961, 0.15294118,\n",
       "        0.15294118, 0.15686275, 0.15686275, 0.16470588, 0.16470588,\n",
       "        0.16862745, 0.16862745, 0.17254902, 0.17254902, 0.18039216,\n",
       "        0.18039216, 0.18431373, 0.18431373, 0.19215686, 0.19215686,\n",
       "        0.20392157, 0.20392157, 0.20784314, 0.20784314, 0.21960784,\n",
       "        0.21960784, 0.22352941, 0.22352941, 0.23137255, 0.23137255,\n",
       "        0.23921569, 0.23921569, 0.24313725, 0.24313725, 0.25882353,\n",
       "        0.25882353, 0.26666667, 0.26666667, 0.27058824, 0.27058824,\n",
       "        0.27843137, 0.27843137, 0.28627451, 0.28627451, 0.29411765,\n",
       "        0.29411765, 0.29803922, 0.29803922, 0.30196078, 0.30196078,\n",
       "        0.30588235, 0.30588235, 0.31372549, 0.31372549, 0.31764706,\n",
       "        0.31764706, 0.3254902 , 0.3254902 , 0.3372549 , 0.3372549 ,\n",
       "        0.35686275, 0.35686275, 0.36470588, 0.36470588, 0.36862745,\n",
       "        0.36862745, 0.37254902, 0.37254902, 0.37647059, 0.37647059,\n",
       "        0.38039216, 0.38039216, 0.39215686, 0.39215686, 0.4       ,\n",
       "        0.4       , 0.40784314, 0.40784314, 0.41960784, 0.41960784,\n",
       "        0.42352941, 0.42352941, 0.43921569, 0.43921569, 0.45490196,\n",
       "        0.45490196, 0.4627451 , 0.4627451 , 0.47058824, 0.47058824,\n",
       "        0.49019608, 0.49019608, 0.49411765, 0.49411765, 0.50980392,\n",
       "        0.50980392, 0.54117647, 0.54117647, 0.55686275, 0.55686275,\n",
       "        0.56078431, 0.56078431, 0.6       , 0.6       , 0.61176471,\n",
       "        0.61176471, 0.65490196, 0.65490196, 0.6627451 , 0.6627451 ,\n",
       "        0.68235294, 0.68235294, 0.70196078, 0.70196078, 0.73333333,\n",
       "        0.73333333, 0.7372549 , 0.7372549 , 0.76470588, 0.76470588,\n",
       "        0.78823529, 0.78823529, 0.87843137, 0.87843137, 0.90588235,\n",
       "        0.90588235, 0.94509804, 0.94509804, 1.        , 1.        ]),\n",
       " array([0.00380228, 0.02661597, 0.02661597, 0.03422053, 0.03422053,\n",
       "        0.03802281, 0.03802281, 0.12927757, 0.12927757, 0.15209125,\n",
       "        0.15209125, 0.22053232, 0.22053232, 0.22813688, 0.22813688,\n",
       "        0.25475285, 0.25475285, 0.25855513, 0.25855513, 0.2661597 ,\n",
       "        0.2661597 , 0.30418251, 0.30418251, 0.30798479, 0.30798479,\n",
       "        0.33840304, 0.33840304, 0.34980989, 0.34980989, 0.35361217,\n",
       "        0.35361217, 0.36121673, 0.36121673, 0.38403042, 0.38403042,\n",
       "        0.39163498, 0.39163498, 0.41444867, 0.41444867, 0.42585551,\n",
       "        0.42585551, 0.46387833, 0.46387833, 0.46768061, 0.46768061,\n",
       "        0.47148289, 0.47148289, 0.50190114, 0.50190114, 0.5095057 ,\n",
       "        0.5095057 , 0.53231939, 0.53231939, 0.54752852, 0.54752852,\n",
       "        0.55513308, 0.55513308, 0.55893536, 0.55893536, 0.57034221,\n",
       "        0.57034221, 0.58174905, 0.58174905, 0.58555133, 0.58555133,\n",
       "        0.58935361, 0.58935361, 0.59315589, 0.59315589, 0.59695817,\n",
       "        0.59695817, 0.60076046, 0.60076046, 0.61596958, 0.61596958,\n",
       "        0.62737643, 0.62737643, 0.63117871, 0.63117871, 0.63878327,\n",
       "        0.63878327, 0.64258555, 0.64258555, 0.6539924 , 0.6539924 ,\n",
       "        0.66539924, 0.66539924, 0.68060837, 0.68060837, 0.68441065,\n",
       "        0.68441065, 0.68821293, 0.68821293, 0.69201521, 0.69201521,\n",
       "        0.69581749, 0.69581749, 0.69961977, 0.69961977, 0.71102662,\n",
       "        0.71102662, 0.7148289 , 0.7148289 , 0.71863118, 0.71863118,\n",
       "        0.7338403 , 0.7338403 , 0.74904943, 0.74904943, 0.75665399,\n",
       "        0.75665399, 0.76045627, 0.76045627, 0.76806084, 0.76806084,\n",
       "        0.77946768, 0.77946768, 0.79087452, 0.79087452, 0.80228137,\n",
       "        0.80228137, 0.80608365, 0.80608365, 0.81368821, 0.81368821,\n",
       "        0.81749049, 0.81749049, 0.82129278, 0.82129278, 0.82509506,\n",
       "        0.82509506, 0.82889734, 0.82889734, 0.83269962, 0.83269962,\n",
       "        0.84030418, 0.84030418, 0.84410646, 0.84410646, 0.85171103,\n",
       "        0.85171103, 0.85931559, 0.85931559, 0.87072243, 0.87072243,\n",
       "        0.88593156, 0.88593156, 0.89353612, 0.89353612, 0.8973384 ,\n",
       "        0.8973384 , 0.90874525, 0.90874525, 0.91254753, 0.91254753,\n",
       "        0.91634981, 0.91634981, 0.92775665, 0.92775665, 0.93155894,\n",
       "        0.93155894, 0.9391635 , 0.9391635 , 0.94676806, 0.94676806,\n",
       "        0.95057034, 0.95057034, 0.95437262, 0.95437262, 0.9581749 ,\n",
       "        0.9581749 , 0.96197719, 0.96197719, 0.96958175, 0.96958175,\n",
       "        0.97338403, 0.97338403, 0.97718631, 0.97718631, 0.98098859,\n",
       "        0.98098859, 0.98479087, 0.98479087, 0.98859316, 0.98859316,\n",
       "        0.99239544, 0.99239544, 0.99619772, 0.99619772, 1.        ]),\n",
       " array([0.94531504, 0.88708103, 0.86723822, 0.85802253, 0.84953606,\n",
       "        0.8464581 , 0.83801289, 0.73851337, 0.73786329, 0.72543192,\n",
       "        0.72519539, 0.67410483, 0.66964698, 0.66586994, 0.66168796,\n",
       "        0.65039645, 0.64868537, 0.64825248, 0.64452388, 0.64267144,\n",
       "        0.64061148, 0.63170612, 0.63114415, 0.62847507, 0.62404402,\n",
       "        0.61237906, 0.61151128, 0.60832242, 0.60396196, 0.60391077,\n",
       "        0.60115491, 0.59990299, 0.59906372, 0.59526733, 0.59429932,\n",
       "        0.59065204, 0.5873905 , 0.58112728, 0.58031755, 0.57777697,\n",
       "        0.57755964, 0.57434647, 0.57382179, 0.57343677, 0.57176708,\n",
       "        0.5717034 , 0.57021464, 0.56595735, 0.56563165, 0.56446284,\n",
       "        0.56442181, 0.55935633, 0.55914144, 0.5568495 , 0.55667286,\n",
       "        0.55547551, 0.55535347, 0.55526381, 0.5546746 , 0.55140433,\n",
       "        0.55039434, 0.54692709, 0.5469178 , 0.54675987, 0.54626729,\n",
       "        0.54401202, 0.54364961, 0.54299539, 0.54167738, 0.54163039,\n",
       "        0.54149417, 0.54145787, 0.54119638, 0.53601028, 0.53244159,\n",
       "        0.5282305 , 0.52761519, 0.52681853, 0.52578008, 0.5248467 ,\n",
       "        0.52263093, 0.52082488, 0.51779484, 0.51627031, 0.51493336,\n",
       "        0.5122508 , 0.51079116, 0.50952367, 0.5071095 , 0.50549999,\n",
       "        0.50404957, 0.50306147, 0.50257289, 0.50124837, 0.49588712,\n",
       "        0.49483794, 0.49332003, 0.49245141, 0.49161715, 0.48969102,\n",
       "        0.4889016 , 0.48790731, 0.48668077, 0.48645121, 0.48550166,\n",
       "        0.4844728 , 0.48274286, 0.47882668, 0.47860445, 0.47830626,\n",
       "        0.47616181, 0.47341084, 0.47320757, 0.47058797, 0.47019274,\n",
       "        0.46711325, 0.46615768, 0.46221144, 0.45941011, 0.45449757,\n",
       "        0.45036046, 0.44935764, 0.44681314, 0.4454717 , 0.44477856,\n",
       "        0.44344741, 0.44251682, 0.44250681, 0.44238309, 0.44189706,\n",
       "        0.44187925, 0.44159508, 0.4372327 , 0.43663514, 0.43608639,\n",
       "        0.43562272, 0.43349609, 0.43339055, 0.43108835, 0.42834651,\n",
       "        0.42775448, 0.42669029, 0.42302362, 0.42052647, 0.41701471,\n",
       "        0.41046296, 0.40780913, 0.40560213, 0.40503522, 0.40484623,\n",
       "        0.40181868, 0.39542045, 0.39394811, 0.39352608, 0.38836711,\n",
       "        0.38777328, 0.38063212, 0.3766176 , 0.37149925, 0.37066739,\n",
       "        0.37033399, 0.36848091, 0.36222725, 0.36174361, 0.35991896,\n",
       "        0.35982745, 0.3464801 , 0.34509705, 0.34350168, 0.34306869,\n",
       "        0.34004045, 0.33838539, 0.33175564, 0.3304189 , 0.32316943,\n",
       "        0.32035423, 0.32021552, 0.31997358, 0.30935268, 0.30902727,\n",
       "        0.30410701, 0.30391172, 0.27047193, 0.26934813, 0.25400829,\n",
       "        0.24588893, 0.22572691, 0.21707439, 0.11465766, 0.05641173]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_curve(x, temp.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_curve_plot(y, y_score):\n",
    "    precision, recall, _ = precision_recall_curve(y, y_score)\n",
    "    area = auc(recall,precision)\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "    \n",
    "    plt.figure(2)\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve: AUC={0:0.2f}'.format(\n",
    "              area))\n",
    "    \n",
    "    string = 'auprc_clinicalbert_'+readmission_mode+'.png'\n",
    "\n",
    "    plt.savefig(os.path.join(output_dir, string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at Precision of 80 is {} 0.5855513307984791\n"
     ]
    }
   ],
   "source": [
    "score = logits_history\n",
    "df_test['pred_score'] = score\n",
    "df_sort = df_test.sort_values(by=['ID'])\n",
    "#score \n",
    "temp = (df_sort.groupby(['ID'])['pred_score'].agg(max)+df_sort.groupby(['ID'])['pred_score'].agg(sum)/2)/(1+df_sort.groupby(['ID'])['pred_score'].agg(len)/2)\n",
    "y = df_sort.groupby(['ID'])['Label'].agg(np.min).values\n",
    "\n",
    "precision, recall, thres = precision_recall_curve(y, temp)\n",
    "pr_thres = pd.DataFrame(data =  list(zip(precision, recall, thres)), columns = ['prec','recall','thres'])\n",
    "vote_df = pd.DataFrame(data =  list(zip(temp, y)), columns = ['score','label'])\n",
    "\n",
    "pr_curve_plot(y, temp)\n",
    "\n",
    "temp = pr_thres[pr_thres.prec > 0.799999].reset_index()\n",
    "\n",
    "rp80 = 0\n",
    "if temp.size == 0:\n",
    "    print('Test Sample too small or RP80=0')\n",
    "else:\n",
    "    rp80 = temp.iloc[0].recall\n",
    "    print('Recall at Precision of 80 is {}', rp80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_pr_curve(df, score):\n",
    "    df['pred_score'] = score\n",
    "    df_sort = df.sort_values(by=['ID'])\n",
    "    #score \n",
    "    temp = (df_sort.groupby(['ID'])['pred_score'].agg(max)+df_sort.groupby(['ID'])['pred_score'].agg(sum)/2)/(1+df_sort.groupby(['ID'])['pred_score'].agg(len)/2)\n",
    "    y = df_sort.groupby(['ID'])['Label'].agg(np.min).values\n",
    "    \n",
    "    precision, recall, thres = precision_recall_curve(y, temp)\n",
    "    pr_thres = pd.DataFrame(data =  list(zip(precision, recall, thres)), columns = ['prec','recall','thres'])\n",
    "    vote_df = pd.DataFrame(data =  list(zip(temp, y)), columns = ['score','label'])\n",
    "    \n",
    "    pr_curve_plot(y, temp)\n",
    "    \n",
    "    temp = pr_thres[pr_thres.prec > 0.799999].reset_index()\n",
    "    \n",
    "    rp80 = 0\n",
    "    if temp.size == 0:\n",
    "        print('Test Sample too small or RP80=0')\n",
    "    else:\n",
    "        rp80 = temp.iloc[0].recall\n",
    "        print('Recall at Precision of 80 is {}', rp80)\n",
    "\n",
    "    return rp80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5082933902740479,\n",
       " 0.7006613612174988,\n",
       " 0.6611330509185791,\n",
       " 0.6493451595306396,\n",
       " 0.9860514402389526,\n",
       " 0.6569519639015198,\n",
       " 0.4432774782180786,\n",
       " 0.4770866632461548,\n",
       " 0.6866414546966553,\n",
       " 0.6759017109870911,\n",
       " 0.6439549326896667,\n",
       " 0.6713142991065979,\n",
       " 0.5895968079566956,\n",
       " 0.6126990914344788,\n",
       " 0.9767186641693115,\n",
       " 0.536064863204956,\n",
       " 0.5978657603263855,\n",
       " 0.5889393091201782,\n",
       " 0.5681677460670471,\n",
       " 0.9861887097358704,\n",
       " 0.6569519639015198,\n",
       " 0.4539942443370819,\n",
       " 0.5060821771621704,\n",
       " 0.5795586109161377,\n",
       " 0.6155515909194946,\n",
       " 0.9234936237335205,\n",
       " 0.6154227256774902,\n",
       " 0.9573972821235657,\n",
       " 0.9049018025398254,\n",
       " 0.9058811664581299,\n",
       " 0.6706217527389526,\n",
       " 0.8597192168235779,\n",
       " 0.8453426361083984,\n",
       " 0.6923359036445618,\n",
       " 0.8799079656600952,\n",
       " 0.8943763375282288,\n",
       " 0.9410731792449951,\n",
       " 0.6368076205253601,\n",
       " 0.6058018207550049,\n",
       " 0.26662394404411316,\n",
       " 0.3256610929965973,\n",
       " 0.14961589872837067,\n",
       " 0.19679449498653412,\n",
       " 0.3032858967781067,\n",
       " 0.6527687907218933,\n",
       " 0.6237432360649109,\n",
       " 0.5560782551765442,\n",
       " 0.5379992723464966,\n",
       " 0.5875970125198364,\n",
       " 0.51422518491745,\n",
       " 0.20121078193187714,\n",
       " 0.18302959203720093,\n",
       " 0.15404407680034637,\n",
       " 0.09469960629940033,\n",
       " 0.3541344404220581,\n",
       " 0.5091691017150879,\n",
       " 0.14794567227363586,\n",
       " 0.19499938189983368,\n",
       " 0.354347825050354,\n",
       " 0.5270136594772339,\n",
       " 0.32896509766578674,\n",
       " 0.33214375376701355,\n",
       " 0.5249388813972473,\n",
       " 0.1600724160671234,\n",
       " 0.3007536232471466,\n",
       " 0.0886852964758873,\n",
       " 0.5411149263381958,\n",
       " 0.11849213391542435,\n",
       " 0.3725389838218689,\n",
       " 0.2482789307832718,\n",
       " 0.38884538412094116,\n",
       " 0.41365382075309753,\n",
       " 0.3449861705303192,\n",
       " 0.5151304006576538,\n",
       " 0.17040936648845673,\n",
       " 0.5257800817489624,\n",
       " 0.16799113154411316,\n",
       " 0.25191715359687805,\n",
       " 0.19060854613780975,\n",
       " 0.46014827489852905,\n",
       " 0.1032789796590805,\n",
       " 0.0943794921040535,\n",
       " 0.07532145828008652,\n",
       " 0.08526870608329773,\n",
       " 0.44682297110557556,\n",
       " 0.4257858991622925,\n",
       " 0.2700543701648712,\n",
       " 0.11665501445531845,\n",
       " 0.5829551219940186,\n",
       " 0.6181703805923462,\n",
       " 0.16673125326633453,\n",
       " 0.3666142523288727,\n",
       " 0.5944626331329346,\n",
       " 0.2695750892162323,\n",
       " 0.1822526752948761,\n",
       " 0.5948746204376221,\n",
       " 0.6271013021469116,\n",
       " 0.19974194467067719,\n",
       " 0.20919597148895264,\n",
       " 0.5787036418914795,\n",
       " 0.5511049032211304,\n",
       " 0.11288999766111374,\n",
       " 0.5066913366317749,\n",
       " 0.19096235930919647,\n",
       " 0.59711092710495,\n",
       " 0.6159723997116089,\n",
       " 0.2837543487548828,\n",
       " 0.14941129088401794,\n",
       " 0.33084383606910706,\n",
       " 0.3936043679714203,\n",
       " 0.2634125053882599,\n",
       " 0.15256725251674652,\n",
       " 0.43554720282554626,\n",
       " 0.3266187608242035,\n",
       " 0.18231628835201263,\n",
       " 0.688784658908844,\n",
       " 0.6149043440818787,\n",
       " 0.12891785800457,\n",
       " 0.06682931631803513,\n",
       " 0.2956077456474304,\n",
       " 0.2945559322834015,\n",
       " 0.4448266625404358,\n",
       " 0.30358439683914185,\n",
       " 0.22491444647312164,\n",
       " 0.3144834637641907,\n",
       " 0.4111863672733307,\n",
       " 0.14970891177654266,\n",
       " 0.6445363163948059,\n",
       " 0.29805564880371094,\n",
       " 0.16860483586788177,\n",
       " 0.6460878252983093,\n",
       " 0.2825024425983429,\n",
       " 0.15285637974739075,\n",
       " 0.642693817615509,\n",
       " 0.23853546380996704,\n",
       " 0.07630069553852081,\n",
       " 0.3177291452884674,\n",
       " 0.24488407373428345,\n",
       " 0.21212682127952576,\n",
       " 0.22289332747459412,\n",
       " 0.2832135260105133,\n",
       " 0.14256148040294647,\n",
       " 0.04004911705851555,\n",
       " 0.203695148229599,\n",
       " 0.1391027867794037,\n",
       " 0.4188534617424011,\n",
       " 0.3548087775707245,\n",
       " 0.26995545625686646,\n",
       " 0.13659250736236572,\n",
       " 0.6403653621673584,\n",
       " 0.2121412456035614,\n",
       " 0.13107436895370483,\n",
       " 0.6265820860862732,\n",
       " 0.23530644178390503,\n",
       " 0.14966562390327454,\n",
       " 0.13926661014556885,\n",
       " 0.5769855976104736,\n",
       " 0.17553144693374634,\n",
       " 0.3279494047164917,\n",
       " 0.4999611973762512,\n",
       " 0.17488831281661987,\n",
       " 0.40139254927635193,\n",
       " 0.38441208004951477,\n",
       " 0.09523516148328781,\n",
       " 0.2737799882888794,\n",
       " 0.34624210000038147,\n",
       " 0.12442672997713089,\n",
       " 0.3311566412448883,\n",
       " 0.02999555878341198,\n",
       " 0.4647851586341858,\n",
       " 0.6722473502159119,\n",
       " 0.07102352380752563,\n",
       " 0.03098214976489544,\n",
       " 0.9604933857917786,\n",
       " 0.771303117275238,\n",
       " 0.5218902230262756,\n",
       " 0.58689945936203,\n",
       " 0.46546342968940735,\n",
       " 0.13470180332660675,\n",
       " 0.26805028319358826,\n",
       " 0.5287509560585022,\n",
       " 0.44481340050697327,\n",
       " 0.24668601155281067,\n",
       " 0.022991444915533066,\n",
       " 0.28884249925613403,\n",
       " 0.08744115382432938,\n",
       " 0.46458324790000916,\n",
       " 0.317726194858551,\n",
       " 0.026603929698467255,\n",
       " 0.17530293762683868,\n",
       " 0.3049936890602112,\n",
       " 0.018070172518491745,\n",
       " 0.7084135413169861,\n",
       " 0.40766802430152893,\n",
       " 0.16946101188659668,\n",
       " 0.17970292270183563,\n",
       " 0.5906240344047546,\n",
       " 0.31340476870536804,\n",
       " 0.015972839668393135,\n",
       " 0.3888946771621704,\n",
       " 0.13181878626346588,\n",
       " 0.4840904474258423,\n",
       " 0.3403022289276123,\n",
       " 0.03406674414873123,\n",
       " 0.31885579228401184,\n",
       " 0.15214797854423523,\n",
       " 0.1610822230577469,\n",
       " 0.024871455505490303,\n",
       " 0.30564385652542114,\n",
       " 0.1611277312040329,\n",
       " 0.046805042773485184,\n",
       " 0.3111218512058258,\n",
       " 0.07687573879957199,\n",
       " 0.03641349822282791,\n",
       " 0.16893333196640015,\n",
       " 0.24161121249198914,\n",
       " 0.3661843538284302,\n",
       " 0.09510380029678345,\n",
       " 0.6688642501831055,\n",
       " 0.6315808296203613,\n",
       " 0.16885815560817719,\n",
       " 0.07717856764793396,\n",
       " 0.34173446893692017,\n",
       " 0.1536979079246521,\n",
       " 0.0813991129398346,\n",
       " 0.2582458257675171,\n",
       " 0.09488637000322342,\n",
       " 0.42472562193870544,\n",
       " 0.07373756915330887,\n",
       " 0.5529624819755554,\n",
       " 0.12963761389255524,\n",
       " 0.1484840363264084,\n",
       " 0.2255895733833313,\n",
       " 0.5128252506256104,\n",
       " 0.13510288298130035,\n",
       " 0.07567231357097626,\n",
       " 0.12381934374570847,\n",
       " 0.15079519152641296,\n",
       " 0.09113627672195435,\n",
       " 0.22970306873321533,\n",
       " 0.45985910296440125,\n",
       " 0.05456968769431114,\n",
       " 0.03245171159505844,\n",
       " 0.06501242518424988,\n",
       " 0.40016448497772217,\n",
       " 0.6621503233909607,\n",
       " 0.39846500754356384,\n",
       " 0.5464279651641846,\n",
       " 0.5047572255134583,\n",
       " 0.6821898818016052,\n",
       " 0.9546682834625244,\n",
       " 0.969520092010498,\n",
       " 0.9842497706413269,\n",
       " 0.9214370250701904,\n",
       " 0.9650851488113403,\n",
       " 0.9771803617477417,\n",
       " 0.9239239692687988,\n",
       " 0.9761731028556824,\n",
       " 0.48951712250709534,\n",
       " 0.9108019471168518,\n",
       " 0.4970194101333618,\n",
       " 0.889910876750946,\n",
       " 0.9499016404151917,\n",
       " 0.7958213686943054,\n",
       " 0.5018972158432007,\n",
       " 0.8638215065002441,\n",
       " 0.7836700677871704,\n",
       " 0.98875892162323,\n",
       " 0.4651601016521454,\n",
       " 0.9082046747207642,\n",
       " 0.8564355373382568,\n",
       " 0.9902417063713074,\n",
       " 0.5046492218971252,\n",
       " 0.9123870730400085,\n",
       " 0.9188385009765625,\n",
       " 0.9900144338607788,\n",
       " 0.573232114315033,\n",
       " 0.6883241534233093,\n",
       " 0.458006352186203,\n",
       " 0.9627286195755005,\n",
       " 0.7025899291038513,\n",
       " 0.9575175046920776,\n",
       " 0.5201233625411987,\n",
       " 0.8048516511917114,\n",
       " 0.8531385064125061,\n",
       " 0.9836241006851196,\n",
       " 0.9178177118301392,\n",
       " 0.9436084032058716,\n",
       " 0.46315398812294006,\n",
       " 0.6742905378341675,\n",
       " 0.827663242816925,\n",
       " 0.9634017944335938,\n",
       " 0.8335444331169128,\n",
       " 0.96165931224823,\n",
       " 0.9660770297050476,\n",
       " 0.9682950377464294,\n",
       " 0.846380889415741,\n",
       " 0.9846440553665161,\n",
       " 0.9907833337783813,\n",
       " 0.8417615294456482,\n",
       " 0.6589770317077637,\n",
       " 0.8855648636817932,\n",
       " 0.9055423140525818,\n",
       " 0.5977632403373718,\n",
       " 0.7116414904594421,\n",
       " 0.5524413585662842,\n",
       " 0.5720058083534241,\n",
       " 0.899215817451477,\n",
       " 0.501459002494812,\n",
       " 0.6039287447929382,\n",
       " 0.6966363787651062,\n",
       " 0.43343037366867065,\n",
       " 0.9568737745285034,\n",
       " 0.9782691597938538,\n",
       " 0.8772130608558655,\n",
       " 0.8333156108856201,\n",
       " 0.4818118214607239,\n",
       " 0.8938360810279846,\n",
       " 0.9524981379508972,\n",
       " 0.48194193840026855,\n",
       " 0.9351962208747864,\n",
       " 0.9123258590698242,\n",
       " 0.9481313228607178,\n",
       " 0.9793590903282166,\n",
       " 0.9000478386878967,\n",
       " 0.9135948419570923,\n",
       " 0.4713577628135681,\n",
       " 0.7868109941482544,\n",
       " 0.9503934383392334,\n",
       " 0.5666229724884033,\n",
       " 0.7823759317398071,\n",
       " 0.9622968435287476,\n",
       " 0.49575528502464294,\n",
       " 0.9770987629890442,\n",
       " 0.7989805936813354,\n",
       " 0.6545364856719971,\n",
       " 0.8080638647079468,\n",
       " 0.6656197905540466,\n",
       " 0.5536591410636902,\n",
       " 0.1448620706796646,\n",
       " 0.20107939839363098,\n",
       " 0.22268494963645935,\n",
       " 0.4996507167816162,\n",
       " 0.37461867928504944,\n",
       " 0.29466328024864197,\n",
       " 0.12209178507328033,\n",
       " 0.36720290780067444,\n",
       " 0.3862602114677429,\n",
       " 0.34978264570236206,\n",
       " 0.8189341425895691,\n",
       " 0.5656267404556274,\n",
       " 0.715092122554779,\n",
       " 0.7386227250099182,\n",
       " 0.6931855082511902,\n",
       " 0.6276995539665222,\n",
       " 0.5290117859840393,\n",
       " 0.7104406356811523,\n",
       " 0.7795568108558655,\n",
       " 0.8281787633895874,\n",
       " 0.7725223898887634,\n",
       " 0.7486333250999451,\n",
       " 0.7365387678146362,\n",
       " 0.677301824092865,\n",
       " 0.6766703724861145,\n",
       " 0.8795065879821777,\n",
       " 0.8419739007949829,\n",
       " 0.9032166004180908,\n",
       " 0.8983570337295532,\n",
       " 0.23293428122997284,\n",
       " 0.1789696365594864,\n",
       " 0.23891021311283112,\n",
       " 0.22955016791820526,\n",
       " 0.4241344630718231,\n",
       " 0.29717719554901123,\n",
       " 0.6185908913612366,\n",
       " 0.3479885458946228,\n",
       " 0.4464080035686493,\n",
       " 0.18576452136039734,\n",
       " 0.4205928444862366,\n",
       " 0.27006295323371887,\n",
       " 0.5153477191925049,\n",
       " 0.3237924575805664,\n",
       " 0.33246636390686035,\n",
       " 0.41386207938194275,\n",
       " 0.8110278844833374,\n",
       " 0.4466555714607239,\n",
       " 0.29972603917121887,\n",
       " 0.17267100512981415,\n",
       " 0.2530975341796875,\n",
       " 0.5172058343887329,\n",
       " 0.416280061006546,\n",
       " 0.5427716374397278,\n",
       " 0.3348809480667114,\n",
       " 0.46964162588119507,\n",
       " 0.3236755132675171,\n",
       " 0.29415494203567505,\n",
       " 0.2641516327857971,\n",
       " 0.24421149492263794,\n",
       " 0.6086289286613464,\n",
       " 0.3484472334384918,\n",
       " 0.6560987830162048,\n",
       " 0.24158866703510284,\n",
       " 0.3233843743801117,\n",
       " 0.6373900175094604,\n",
       " 0.2710168957710266,\n",
       " 0.6632817983627319,\n",
       " 0.5902954339981079,\n",
       " 0.49843528866767883,\n",
       " 0.5176441073417664,\n",
       " 0.5965301990509033,\n",
       " 0.6595882177352905,\n",
       " 0.26192861795425415,\n",
       " 0.6941534280776978,\n",
       " 0.45865222811698914,\n",
       " 0.6425251364707947,\n",
       " 0.08246061205863953,\n",
       " 0.2890293300151825,\n",
       " 0.6270182132720947,\n",
       " 0.6458112001419067,\n",
       " 0.7947074770927429,\n",
       " 0.4255932867527008,\n",
       " 0.6215968132019043,\n",
       " 0.7063260674476624,\n",
       " 0.21151575446128845,\n",
       " 0.4556109309196472,\n",
       " 0.6235755085945129,\n",
       " 0.6542341113090515,\n",
       " 0.5394063591957092,\n",
       " 0.7995156049728394,\n",
       " 0.3945334553718567,\n",
       " 0.6380035877227783,\n",
       " 0.8135015964508057,\n",
       " 0.7534853219985962,\n",
       " 0.6457704305648804,\n",
       " 0.541013777256012,\n",
       " 0.774657666683197,\n",
       " 0.8178041577339172,\n",
       " 0.5652703642845154,\n",
       " 0.8877171277999878,\n",
       " 0.48961037397384644,\n",
       " 0.8746921420097351,\n",
       " 0.6026647686958313,\n",
       " 0.2913925051689148,\n",
       " 0.23639139533042908,\n",
       " 0.3415692150592804,\n",
       " 0.31562331318855286,\n",
       " 0.3107430338859558,\n",
       " 0.2760537564754486,\n",
       " 0.21770501136779785,\n",
       " 0.4945126473903656,\n",
       " 0.07305116206407547,\n",
       " 0.3300062119960785,\n",
       " 0.5996286869049072,\n",
       " 0.8289949893951416,\n",
       " 0.7803657650947571,\n",
       " 0.6955186128616333,\n",
       " 0.496242493391037,\n",
       " 0.5358676314353943,\n",
       " 0.6093876361846924,\n",
       " 0.469992995262146,\n",
       " 0.6075745224952698,\n",
       " 0.7334372401237488,\n",
       " 0.4125148355960846,\n",
       " 0.46304240822792053,\n",
       " 0.4085915982723236,\n",
       " 0.7270114421844482,\n",
       " 0.6880598664283752,\n",
       " 0.36514994502067566,\n",
       " 0.44189706444740295,\n",
       " 0.13244876265525818,\n",
       " 0.07626646012067795,\n",
       " 0.31824830174446106,\n",
       " 0.37971097230911255,\n",
       " 0.23572754859924316,\n",
       " 0.30483582615852356,\n",
       " 0.27814239263534546,\n",
       " 0.8212195038795471,\n",
       " 0.7171242833137512,\n",
       " 0.9749602675437927,\n",
       " 0.7161335349082947,\n",
       " 0.7673943042755127,\n",
       " 0.48231813311576843,\n",
       " 0.49433255195617676,\n",
       " 0.5150309801101685,\n",
       " 0.4119870364665985,\n",
       " 0.3928355276584625,\n",
       " 0.7448244094848633,\n",
       " 0.6918805241584778,\n",
       " 0.518666684627533,\n",
       " 0.5411849021911621,\n",
       " 0.5752202272415161,\n",
       " 0.9314364790916443,\n",
       " 0.6037683486938477,\n",
       " 0.4243917167186737,\n",
       " 0.669912576675415,\n",
       " 0.3747122287750244,\n",
       " 0.5388752818107605,\n",
       " 0.4499349892139435,\n",
       " 0.4349019527435303,\n",
       " 0.6778845191001892,\n",
       " 0.3841797411441803,\n",
       " 0.32492929697036743,\n",
       " 0.1927630603313446,\n",
       " 0.5730865597724915,\n",
       " 0.44011765718460083,\n",
       " 0.29075366258621216,\n",
       " 0.36187151074409485,\n",
       " 0.703838050365448,\n",
       " 0.5028688907623291,\n",
       " 0.3849155008792877,\n",
       " 0.2929900288581848,\n",
       " 0.78849858045578,\n",
       " 0.5022370219230652,\n",
       " 0.3511492908000946,\n",
       " 0.8131479024887085,\n",
       " 0.5755018591880798,\n",
       " 0.38642948865890503,\n",
       " 0.4379090666770935,\n",
       " 0.6926308870315552,\n",
       " 0.4671194553375244,\n",
       " 0.2172393500804901,\n",
       " 0.2654384672641754,\n",
       " 0.17439796030521393,\n",
       " 0.22590553760528564,\n",
       " 0.47754907608032227,\n",
       " 0.2304609715938568,\n",
       " 0.12308904528617859,\n",
       " 0.05709626525640488,\n",
       " 0.08553475141525269,\n",
       " 0.2376275509595871,\n",
       " 0.023484129458665848,\n",
       " 0.08248179405927658,\n",
       " 0.1460675597190857,\n",
       " 0.08946569263935089,\n",
       " 0.18266655504703522,\n",
       " 0.08608271181583405,\n",
       " 0.39227840304374695,\n",
       " 0.18578076362609863,\n",
       " 0.03005366213619709,\n",
       " 0.5905072689056396,\n",
       " 0.22145861387252808,\n",
       " 0.03137385845184326,\n",
       " 0.15489427745342255,\n",
       " 0.20128482580184937,\n",
       " 0.023768259212374687,\n",
       " 0.093279168009758,\n",
       " 0.4712800681591034,\n",
       " 0.10265274345874786,\n",
       " 0.04197631776332855,\n",
       " 0.15299975872039795,\n",
       " 0.08003587275743484,\n",
       " 0.1373448520898819,\n",
       " 0.07550361752510071,\n",
       " 0.13673226535320282,\n",
       " 0.18519894778728485,\n",
       " 0.2774800956249237,\n",
       " 0.18709713220596313,\n",
       " 0.03269844129681587,\n",
       " 0.20057077705860138,\n",
       " 0.4288828372955322,\n",
       " 0.34166064858436584,\n",
       " 0.258686900138855,\n",
       " 0.02386609837412834,\n",
       " 0.5383674502372742,\n",
       " 0.22581085562705994,\n",
       " 0.02519066631793976,\n",
       " 0.17680326104164124,\n",
       " 0.2278103083372116,\n",
       " 0.31538477540016174,\n",
       " 0.4017705023288727,\n",
       " 0.42572978138923645,\n",
       " 0.49176517128944397,\n",
       " 0.19308649003505707,\n",
       " 0.0280153825879097,\n",
       " 0.04855215176939964,\n",
       " 0.6373429894447327,\n",
       " 0.3937146067619324,\n",
       " 0.04262027144432068,\n",
       " 0.3607231378555298,\n",
       " 0.15584759414196014,\n",
       " 0.09132196009159088,\n",
       " 0.030403804033994675,\n",
       " 0.22328422963619232,\n",
       " 0.3922837972640991,\n",
       " 0.041642844676971436,\n",
       " 0.5929328799247742,\n",
       " 0.4140743613243103,\n",
       " 0.03223045542836189,\n",
       " 0.08135083317756653,\n",
       " 0.051805056631565094,\n",
       " 0.038831762969493866,\n",
       " 0.03196687623858452,\n",
       " 0.047995675355196,\n",
       " 0.20263218879699707,\n",
       " 0.5260206460952759,\n",
       " 0.5476685166358948,\n",
       " 0.6579803824424744,\n",
       " 0.6228784322738647,\n",
       " 0.7571229338645935,\n",
       " 0.5138475894927979,\n",
       " 0.6051214933395386,\n",
       " 0.5225576758384705,\n",
       " 0.4591805040836334,\n",
       " 0.4108729958534241,\n",
       " 0.25609275698661804,\n",
       " 0.394376277923584,\n",
       " 0.43747037649154663,\n",
       " 0.4999098479747772,\n",
       " 0.4895782470703125,\n",
       " 0.6667354702949524,\n",
       " 0.933539867401123,\n",
       " 0.9183017015457153,\n",
       " 0.7339270114898682,\n",
       " 0.49575769901275635,\n",
       " 0.5018399953842163,\n",
       " 0.5511162281036377,\n",
       " 0.35216131806373596,\n",
       " 0.5215823650360107,\n",
       " 0.5672825574874878,\n",
       " 0.25848039984703064,\n",
       " 0.5468938946723938,\n",
       " 0.4167855978012085,\n",
       " 0.47913047671318054,\n",
       " 0.4884311556816101,\n",
       " 0.7753021717071533,\n",
       " 0.6457157135009766,\n",
       " 0.520574688911438,\n",
       " 0.3049318492412567,\n",
       " 0.3927413523197174,\n",
       " 0.4198838770389557,\n",
       " 0.38694313168525696,\n",
       " 0.6192759275436401,\n",
       " 0.3976900279521942,\n",
       " 0.8289017081260681,\n",
       " 0.4646543264389038,\n",
       " 0.39599308371543884,\n",
       " 0.4322148263454437,\n",
       " 0.43596261739730835,\n",
       " 0.41340550780296326,\n",
       " 0.5037124752998352,\n",
       " 0.44383952021598816,\n",
       " 0.8301673531532288,\n",
       " 0.7294109463691711,\n",
       " 0.5614048838615417,\n",
       " 0.7068864107131958,\n",
       " 0.5065027475357056,\n",
       " 0.43513810634613037,\n",
       " 0.5527387857437134,\n",
       " 0.45136404037475586,\n",
       " 0.17484532296657562,\n",
       " 0.42667004466056824,\n",
       " 0.5239366888999939,\n",
       " 0.4617348313331604,\n",
       " 0.2966458797454834,\n",
       " 0.3381626307964325,\n",
       " 0.9952568411827087,\n",
       " 0.942916214466095,\n",
       " 0.981751024723053,\n",
       " 0.8799856305122375,\n",
       " 0.9864615797996521,\n",
       " 0.979900062084198,\n",
       " 0.7041822671890259,\n",
       " 0.9952202439308167,\n",
       " 0.8676723837852478,\n",
       " 0.989831268787384,\n",
       " 0.8841718435287476,\n",
       " 0.717951774597168,\n",
       " 0.7350797057151794,\n",
       " 0.954566478729248,\n",
       " 0.970122754573822,\n",
       " 0.9856280088424683,\n",
       " 0.6464685797691345,\n",
       " 0.9783892631530762,\n",
       " 0.7235963940620422,\n",
       " 0.9958142638206482,\n",
       " 0.9714480042457581,\n",
       " 0.9003682136535645,\n",
       " 0.9830795526504517,\n",
       " 0.9849538803100586,\n",
       " 0.9915032386779785,\n",
       " 0.9816434979438782,\n",
       " 0.9918819069862366,\n",
       " 0.9919297695159912,\n",
       " 0.8399977087974548,\n",
       " 0.7298961281776428,\n",
       " 0.9944855570793152,\n",
       " 0.900649905204773,\n",
       " 0.7005528807640076,\n",
       " 0.9901655316352844,\n",
       " 0.9721804857254028,\n",
       " 0.9674450755119324,\n",
       " 0.5666176676750183,\n",
       " 0.7323001623153687,\n",
       " 0.7288766503334045,\n",
       " 0.6694257259368896,\n",
       " 0.6405226588249207,\n",
       " 0.5150840878486633,\n",
       " 0.9186267256736755,\n",
       " 0.761289119720459,\n",
       " 0.7176598310470581,\n",
       " 0.6661229729652405,\n",
       " 0.9096922874450684,\n",
       " 0.6589657664299011,\n",
       " 0.6466017961502075,\n",
       " 0.8739725351333618,\n",
       " 0.6919755339622498,\n",
       " 0.8831121325492859,\n",
       " 0.21206820011138916,\n",
       " 0.35720294713974,\n",
       " 0.10202829539775848,\n",
       " 0.2516011893749237,\n",
       " 0.2699257433414459,\n",
       " 0.2844511866569519,\n",
       " 0.22933407127857208,\n",
       " 0.3590330183506012,\n",
       " 0.3898555338382721,\n",
       " 0.2763879597187042,\n",
       " 0.2593829929828644,\n",
       " 0.34946516156196594,\n",
       " 0.6686435341835022,\n",
       " 0.5397723913192749,\n",
       " 0.3334435820579529,\n",
       " 0.4453926086425781,\n",
       " 0.5651178359985352,\n",
       " 0.2545921504497528,\n",
       " 0.3974607586860657,\n",
       " 0.1542178988456726,\n",
       " 0.26132163405418396,\n",
       " 0.17816895246505737,\n",
       " 0.1692417562007904,\n",
       " 0.34843212366104126,\n",
       " 0.14299264550209045,\n",
       " 0.562582790851593,\n",
       " 0.7989541292190552,\n",
       " 0.6462340354919434,\n",
       " 0.5006041526794434,\n",
       " 0.028652768582105637,\n",
       " 0.4246196746826172,\n",
       " 0.04966193065047264,\n",
       " 0.43192967772483826,\n",
       " 0.06880897283554077,\n",
       " 0.02751646749675274,\n",
       " 0.043724175542593,\n",
       " 0.20617352426052094,\n",
       " 0.9493259787559509,\n",
       " 0.028325725346803665,\n",
       " 0.04540073871612549,\n",
       " 0.2297954559326172,\n",
       " 0.9409188032150269,\n",
       " 0.028981341049075127,\n",
       " 0.05786716938018799,\n",
       " 0.19688214361667633,\n",
       " 0.44793349504470825,\n",
       " 0.04459182545542717,\n",
       " 0.5559408664703369,\n",
       " 0.036021508276462555,\n",
       " 0.0355638787150383,\n",
       " 0.4890660047531128,\n",
       " 0.048432379961013794,\n",
       " 0.45349615812301636,\n",
       " 0.13126996159553528,\n",
       " 0.06560973078012466,\n",
       " 0.09451578557491302,\n",
       " 0.7410886287689209,\n",
       " 0.7569311261177063,\n",
       " 0.16620586812496185,\n",
       " 0.46335354447364807,\n",
       " 0.6688719391822815,\n",
       " 0.4612268805503845,\n",
       " 0.33690083026885986,\n",
       " 0.7023137807846069,\n",
       " 0.7495934963226318,\n",
       " 0.6372601985931396,\n",
       " 0.07872121036052704,\n",
       " 0.8720602989196777,\n",
       " 0.6598480939865112,\n",
       " 0.3154900074005127,\n",
       " 0.3157367408275604,\n",
       " 0.6803610920906067,\n",
       " 0.15635769069194794,\n",
       " 0.21066410839557648,\n",
       " 0.5802826881408691,\n",
       " 0.547387957572937,\n",
       " 0.9230615496635437,\n",
       " 0.30602335929870605,\n",
       " 0.458443284034729,\n",
       " 0.3924171030521393,\n",
       " 0.6709588170051575,\n",
       " 0.5486972332000732,\n",
       " 0.2130129635334015,\n",
       " 0.8243091702461243,\n",
       " 0.4016399085521698,\n",
       " 0.2903130054473877,\n",
       " 0.28991803526878357,\n",
       " 0.46617332100868225,\n",
       " 0.5641969442367554,\n",
       " 0.45243337750434875,\n",
       " 0.4088151752948761,\n",
       " 0.5821628570556641,\n",
       " 0.380013108253479,\n",
       " 0.5806761384010315,\n",
       " 0.5974156260490417,\n",
       " 0.7553557753562927,\n",
       " 0.6326643228530884,\n",
       " 0.5786831974983215,\n",
       " 0.7030750513076782,\n",
       " 0.14489440619945526,\n",
       " 0.7151100039482117,\n",
       " 0.5693225264549255,\n",
       " 0.45636799931526184,\n",
       " 0.49707064032554626,\n",
       " 0.38214266300201416,\n",
       " 0.6073400974273682,\n",
       " 0.5672212839126587,\n",
       " 0.44547170400619507,\n",
       " 0.4164523780345917,\n",
       " 0.39991632103919983,\n",
       " 0.36665451526641846,\n",
       " 0.2992624342441559,\n",
       " 0.5593563318252563,\n",
       " 0.1152058020234108,\n",
       " 0.1937476396560669,\n",
       " 0.22387096285820007,\n",
       " 0.3610881268978119,\n",
       " 0.665381669998169,\n",
       " 0.6957706809043884,\n",
       " 0.7118070125579834,\n",
       " 0.40817347168922424,\n",
       " 0.37927016615867615,\n",
       " 0.2646551728248596,\n",
       " 0.1879248470067978,\n",
       " 0.15296277403831482,\n",
       " 0.16063649952411652,\n",
       " 0.342014342546463,\n",
       " 0.3660878837108612,\n",
       " 0.3005846440792084,\n",
       " 0.36000409722328186,\n",
       " 0.37030988931655884,\n",
       " 0.39907267689704895,\n",
       " 0.36165380477905273,\n",
       " 0.3229432702064514,\n",
       " 0.2711351215839386,\n",
       " 0.3442309498786926,\n",
       " 0.24577809870243073,\n",
       " 0.48208141326904297,\n",
       " 0.7122094631195068,\n",
       " 0.3527867794036865,\n",
       " 0.11092482507228851,\n",
       " 0.4296641945838928,\n",
       " 0.3717309236526489,\n",
       " 0.2080399990081787,\n",
       " 0.06749072670936584,\n",
       " 0.298239141702652,\n",
       " 0.49046698212623596,\n",
       " 0.2724754810333252,\n",
       " 0.2554863393306732,\n",
       " 0.24442410469055176,\n",
       " 0.28999656438827515,\n",
       " 0.43324047327041626,\n",
       " 0.4760036766529083,\n",
       " 0.39575764536857605,\n",
       " 0.38629838824272156,\n",
       " 0.2982649803161621,\n",
       " 0.43108662962913513,\n",
       " 0.19619420170783997,\n",
       " 0.12812933325767517,\n",
       " 0.3408783972263336,\n",
       " 0.15003836154937744,\n",
       " 0.14342761039733887,\n",
       " 0.0670178011059761,\n",
       " 0.1574961543083191,\n",
       " 0.06813858449459076,\n",
       " 0.38206058740615845,\n",
       " 0.0809473842382431,\n",
       " 0.18203292787075043,\n",
       " 0.07674853503704071,\n",
       " 0.11067559570074081,\n",
       " 0.07616766542196274,\n",
       " 0.3856143355369568,\n",
       " 0.6205921173095703,\n",
       " 0.6255777478218079,\n",
       " 0.05763853341341019,\n",
       " 0.35362887382507324,\n",
       " 0.5368974208831787,\n",
       " 0.5614053606987,\n",
       " 0.06476616859436035,\n",
       " 0.3325961232185364,\n",
       " 0.2949739098548889,\n",
       " 0.3719624876976013,\n",
       " 0.18373724818229675,\n",
       " 0.20730704069137573,\n",
       " 0.1388651579618454,\n",
       " 0.16036757826805115,\n",
       " 0.21191738545894623,\n",
       " 0.24010808765888214,\n",
       " 0.4498275816440582,\n",
       " 0.5263928771018982,\n",
       " 0.4893776774406433,\n",
       " 0.25410932302474976,\n",
       " 0.2334120273590088,\n",
       " 0.5902771353721619,\n",
       " 0.5564888715744019,\n",
       " 0.5407767295837402,\n",
       " 0.40041691064834595,\n",
       " 0.2335195243358612,\n",
       " 0.2892773747444153,\n",
       " 0.956320583820343,\n",
       " 0.8639224767684937,\n",
       " 0.4015582501888275,\n",
       " 0.818727970123291,\n",
       " 0.8575966358184814,\n",
       " 0.9523913860321045,\n",
       " 0.9412417411804199,\n",
       " 0.8776066899299622,\n",
       " 0.836401104927063,\n",
       " 0.8531709313392639,\n",
       " 0.8965432047843933,\n",
       " 0.4187123775482178,\n",
       " 0.9840942025184631,\n",
       " 0.9307609796524048,\n",
       " 0.9885651469230652,\n",
       " 0.8291206955909729,\n",
       " 0.3719985783100128,\n",
       " 0.9666926860809326,\n",
       " 0.34366458654403687,\n",
       " 0.9915042519569397,\n",
       " 0.8965432047843933,\n",
       " 0.4187123775482178,\n",
       " 0.9325082302093506,\n",
       " 0.5067690014839172,\n",
       " 0.24165935814380646,\n",
       " 0.28102150559425354,\n",
       " 0.3445546627044678,\n",
       " 0.6328480243682861,\n",
       " 0.5746787190437317,\n",
       " 0.6055418252944946,\n",
       " 0.24591092765331268,\n",
       " 0.27190783619880676,\n",
       " 0.46357056498527527,\n",
       " 0.27747997641563416,\n",
       " 0.6043689846992493,\n",
       " 0.4681583344936371,\n",
       " 0.6427308320999146,\n",
       " 0.4692246615886688,\n",
       " 0.6326096653938293,\n",
       " 0.4355400800704956,\n",
       " 0.569969654083252,\n",
       " 0.20530833303928375,\n",
       " 0.32301682233810425,\n",
       " 0.21387000381946564,\n",
       " 0.17915353178977966,\n",
       " 0.14687111973762512,\n",
       " 0.1522798091173172,\n",
       " 0.18295620381832123,\n",
       " 0.6185400485992432,\n",
       " 0.2756158709526062,\n",
       " 0.30789443850517273,\n",
       " 0.24044683575630188,\n",
       " 0.3987160921096802,\n",
       " 0.5060503482818604,\n",
       " 0.471963107585907,\n",
       " 0.8245086669921875,\n",
       " 0.46503013372421265,\n",
       " 0.23964780569076538,\n",
       " 0.9550616145133972,\n",
       " 0.17930296063423157,\n",
       " 0.5432517528533936,\n",
       " 0.4279250502586365,\n",
       " 0.47949522733688354,\n",
       " 0.24413922429084778,\n",
       " 0.18556872010231018,\n",
       " 0.14007820188999176,\n",
       " 0.23457185924053192,\n",
       " 0.15702380239963531,\n",
       " 0.11307915300130844,\n",
       " 0.08006879687309265,\n",
       " 0.4847365915775299,\n",
       " 0.2427038699388504,\n",
       " 0.3265847861766815,\n",
       " 0.3915092647075653,\n",
       " 0.11212257295846939,\n",
       " 0.10331186652183533,\n",
       " 0.5610061287879944,\n",
       " 0.47447994351387024,\n",
       " 0.27193155884742737,\n",
       " 0.16401691734790802,\n",
       " 0.1312951147556305,\n",
       " 0.37567728757858276,\n",
       " 0.17438076436519623,\n",
       " 0.10189609229564667,\n",
       " 0.107775017619133,\n",
       " 0.26305341720581055,\n",
       " 0.41858211159706116,\n",
       " 0.5437472462654114,\n",
       " 0.2152029126882553,\n",
       " 0.18722181022167206,\n",
       " 0.14577488601207733,\n",
       " 0.35042503476142883,\n",
       " 0.41003236174583435,\n",
       " 0.3824133574962616,\n",
       " ...]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Label</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>sinus tachycardia. generalized low voltage. de...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.508293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>537</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>preoperative films for liver transplant. techn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>538</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>tip of the left transjugular swan-ganz cathete...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>539</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>transplant/sicu teams aware of huo or lack the...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.649345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540</td>\n",
       "      <td>100133.0</td>\n",
       "      <td>hct twenty-six.two% with four hundred and thir...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.986051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>401376</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>for obstruction. nursing progress note full co...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.297631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5683</th>\n",
       "      <td>401377</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>also displayed. ct of the chest with and witho...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.543576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5684</th>\n",
       "      <td>401378</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>abdomen. ct of the pelvis with and without iv ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5685</th>\n",
       "      <td>401379</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>totally drained 3800cc. later around two thous...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.489785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5686</th>\n",
       "      <td>401380</td>\n",
       "      <td>199930.0</td>\n",
       "      <td>stable post-op. no further bleeding w/ stable ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.329475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5687 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        ID                                               TEXT  \\\n",
       "0            536  100133.0  sinus tachycardia. generalized low voltage. de...   \n",
       "1            537  100133.0  preoperative films for liver transplant. techn...   \n",
       "2            538  100133.0  tip of the left transjugular swan-ganz cathete...   \n",
       "3            539  100133.0  transplant/sicu teams aware of huo or lack the...   \n",
       "4            540  100133.0  hct twenty-six.two% with four hundred and thir...   \n",
       "...          ...       ...                                                ...   \n",
       "5682      401376  199930.0  for obstruction. nursing progress note full co...   \n",
       "5683      401377  199930.0  also displayed. ct of the chest with and witho...   \n",
       "5684      401378  199930.0  abdomen. ct of the pelvis with and without iv ...   \n",
       "5685      401379  199930.0  totally drained 3800cc. later around two thous...   \n",
       "5686      401380  199930.0  stable post-op. no further bleeding w/ stable ...   \n",
       "\n",
       "      Label  pred_score  \n",
       "0       1.0    0.508293  \n",
       "1       1.0    0.700661  \n",
       "2       1.0    0.661133  \n",
       "3       1.0    0.649345  \n",
       "4       1.0    0.986051  \n",
       "...     ...         ...  \n",
       "5682    0.0    0.297631  \n",
       "5683    0.0    0.543576  \n",
       "5684    0.0    0.511333  \n",
       "5685    0.0    0.489785  \n",
       "5686    0.0    0.329475  \n",
       "\n",
       "[5687 rows x 5 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_curve(y, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_thres[pr_thres.prec > 0.799999].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pr_thres[pr_thres.prec > 0.799999].reset_index()\n",
    "rp80 = 0\n",
    "if temp.size == 0:\n",
    "    print('Test Sample too small or RP80=0')\n",
    "else:\n",
    "    rp80 = temp.iloc[0].recall\n",
    "    print('Recall at Precision of 80 is {}', rp80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=100000\n",
    "number_training_steps=1\n",
    "global_step_check=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/haotiang/notebook/logits_clinicalbert_early_chunks.csv'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = '/home/haotiang/notebook'\n",
    "readmission_mode = 'early'\n",
    "string = 'logits_clinicalbert_'+readmission_mode+'_chunks.csv'\n",
    "\n",
    "os.path.join(output_dir, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(output_dir, string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at Precision of 80 is {} 0.5855513307984791\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
    "fpr, tpr, df_out = vote_score(df_test, logits_history)\n",
    "\n",
    "string = 'logits_clinicalbert_'+readmission_mode+'_readmissions.csv'\n",
    "df_out.to_csv(os.path.join(output_dir,string))\n",
    "\n",
    "rp80 = vote_pr_curve(df_test, logits_history)\n",
    "\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy,                 \n",
    "          'global_step': global_step_check,\n",
    "          'training loss': train_loss/number_training_steps,\n",
    "          'RP80': rp80}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RP80': 0.5855513307984791,\n",
       " 'eval_accuracy': 0.7100404431158783,\n",
       " 'eval_loss': 0.5364960494651636,\n",
       " 'global_step': 0,\n",
       " 'training loss': 100000.0}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
